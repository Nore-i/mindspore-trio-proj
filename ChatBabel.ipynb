{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3c04d64",
   "metadata": {},
   "source": [
    "### current directory: /home/lzc/mindspore/ChatBabel.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b34820db-8267-4795-9037-c6a9d8273fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/data1/model/bge1_5-large-zh'\n",
    "llm_path = '/data1/model/qwen1_5-7b-chat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f37e253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun  4 06:59:07 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 6000 Ada Gene...    Off |   00000000:3B:00.0 Off |                  Off |\n",
      "| 30%   30C    P8             26W /  300W |   38975MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX 6000 Ada Gene...    Off |   00000000:5E:00.0 Off |                  Off |\n",
      "| 30%   30C    P8             27W /  300W |   26397MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA RTX 6000 Ada Gene...    Off |   00000000:B1:00.0 Off |                  Off |\n",
      "| 30%   29C    P8             21W /  300W |       4MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA RTX 6000 Ada Gene...    Off |   00000000:D9:00.0 Off |                  Off |\n",
      "| 30%   30C    P8             28W /  300W |    1039MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   1962785      C   ...zc/.conda/envs/workspace/bin/python      38006MiB |\n",
      "|    0   N/A  N/A   2788981      C   ...e/lzc/.conda/envs/huawei/bin/python        958MiB |\n",
      "|    1   N/A  N/A   1899540      C   /home/ah/.conda/envs/ah/bin/python          26390MiB |\n",
      "|    3   N/A  N/A   2788981      C   ...e/lzc/.conda/envs/huawei/bin/python       1032MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c794505",
   "metadata": {},
   "source": [
    "## Preparing papers\n",
    "1. Locate the zip file that contains the papers and unzip them into the `./data` repository.\n",
    "2. Manually create a .bib file that contains all the metadata for the papers and store them in `./bib_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9915ec49-c7d2-419b-b5d4-8b3613c02f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture captured_output\n",
    "# !unzip papers_condensed.zip -d ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b88db04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotDict(dict):\n",
    "    def __getattr__(self, key):\n",
    "        try:\n",
    "            return self[key]\n",
    "        except KeyError:\n",
    "            raise AttributeError(f\"'DotDict' object has no attribute '{key}'\")\n",
    "\n",
    "    def __setattr__(self, key, value):\n",
    "        self[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a750c9-94b8-48e2-ac0f-4181bbc1b9ac",
   "metadata": {},
   "source": [
    "# 1. Preparation for PDF loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d10f3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://hf-mirror.com/bert-base-uncased/resolve/main/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "from multitask_classifier import *\n",
    "from utils import *\n",
    "from utils_rag import *\n",
    "from tokenizer import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "config = {'hidden_dropout_prob': 0.3,\n",
    "\t\t\t'num_labels': {3: 0, 4: 1, 2: 2, 1: 3, 0: 4},\n",
    "\t\t\t'hidden_size': 768,\n",
    "\t\t\t'data_dir': '.',\n",
    "\t\t\t'option': 'finetune'}\n",
    "\n",
    "config = SimpleNamespace(**config)\n",
    "\n",
    "argpath = './finetune-5-1e-05-multitask-final-2.pt'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:2')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "saved = torch.load(argpath)\n",
    "\n",
    "bert_model = MultitaskBERT(config)\n",
    "add_lora_layers(bert_model)\n",
    "for name, param in bert_model.named_parameters():\n",
    "\tif \"lora\" not in name and \"classifier\" not in name and \"bias\" not in name:\n",
    "\t\tparam.requires_grad = False\n",
    "\telse:\n",
    "\t\tparam.requires_grad = True\n",
    "bert_model.load_state_dict(saved['model'])\n",
    "bert_model = bert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b044c390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.9694808125495911\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"gradient-based/evolutionary relay hybrid for computing pareto front approximations maximizing the s-metric\"\n",
    "sentence2 = \"\"\"\n",
    "muiltiobj ective optimization using nondominated sorting in genetic algorithms\n",
    "\"\"\"\n",
    "\n",
    "print(detect_paraphrase(bert_model, device, tokenizer, sentence1, sentence2))\n",
    "print(detect_similarity(bert_model, device, tokenizer, sentence1, sentence2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7ad1b1d-bbd1-4410-a7d6-92c6541419de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A Survey on Evolutionary Computation for Computer Vision and Image Analysis: Past, Present, and Future Trends', 'Bi, Ying')\n",
      "('Modified Distance Calculation in Generational Distance and Inverted Generational Distance', 'Gaspar-Cunha, Ant√≥nio')\n",
      "('Gradient-Based/Evolutionary Relay Hybrid for Computing Pareto Front Approximations Maximizing the S-Metric', 'Bartz-Beielstein, Thomas')\n",
      "('The Pareto archived evolution strategy: a new baseline algorithm for Pareto multiobjective optimisation', 'Knowles, J.')\n",
      "('Enhanced decomposition-based hybrid evolutionary and gradient-based algorithm for many-objective optimization', 'Mohammad Zadeh, Parviz')\n",
      "('Computing Hypervolume Contributions in Low Dimensions: Asymptotically Optimal Algorithm and Complexity Results', 'Takahashi, Ricardo H. C.')\n",
      "('A Scalable Multi-objective Test Problem Toolkit', 'Coello Coello, Carlos A.')\n",
      "('Enhanced decomposition-based hybrid evolutionary and gradient-based algorithm for many-objective optimization', 'Mohammad Zadeh, Parviz')\n",
      "('Performance of Decomposition-Based Many-Objective Algorithms Strongly Depends on Pareto Front Shapes', 'Ishibuchi, Hisao')\n",
      "('The CMA Evolution Strategy: A Tutorial', 'Hansen, Nikolaus')\n",
      "('MOEA/D: A Multiobjective Evolutionary Algorithm Based on Decomposition', '{Qingfu Zhang}')\n",
      "('Gradient-Based/Evolutionary Relay Hybrid for Computing Pareto Front Approximations Maximizing the S-Metric', 'Bartz-Beielstein, Thomas')\n",
      "('Comparison of Multiobjective Evolutionary Algorithms: Empirical Results', 'Zitzler, Eckart')\n",
      "('A parallel global multiobjective framework for optimization: pagmo', 'Biscani, Francesco')\n",
      "('SPEA2: Improving the strength pareto evolutionary algorithm', 'Zitzler, Eckart')\n",
      "('Muiltiobjective Optimization Using Nondominated Sorting in Genetic Algorithms', 'Srinivas, N.')\n",
      "('A Review on Bilevel Optimization: From Classical to Evolutionary Approaches and Applications', 'Sinha, Ankur')\n",
      "('A multi-objective genetic local search algorithm and its application to flowshop scheduling', 'Ishibuchi, H.')\n",
      "('A Niched-Penalty Approach for Constraint Handling in Genetic Algorithms', 'Deb, Kalyanmoy')\n",
      "('SMS-EMOA: Multiobjective selection based on dominated hypervolume', 'Beume, Nicola')\n",
      "('Gradient-Based/Evolutionary Relay Hybrid for Computing Pareto Front Approximations Maximizing the S-Metric', 'Bartz-Beielstein, Thomas')\n",
      "('Theory of the hypervolume indicator: optimal Œº-distributions and the choice of the reference point', 'Auger, Anne')\n",
      "('HypE: An Algorithm for Fast Hypervolume-Based Many-Objective Optimization', 'Bader, Johannes')\n",
      "('Unknown title', 'Unknown author')\n",
      "('Multiobjective evolutionary algorithms: a comparative case study and the strength Pareto approach', 'Zitzler, E.')\n",
      "('Simulated Binary Crossover for Continuous Search Space', 'Deb, Kalyanmoy')\n",
      "('A unified model for multi-objective evolutionary algorithms with elitism', 'Laumanns, M.')\n",
      "('Gradient-Based/Evolutionary Relay Hybrid for Computing Pareto Front Approximations Maximizing the S-Metric', 'Bartz-Beielstein, Thomas')\n",
      "('A multi-objective genetic local search algorithm and its application to flowshop scheduling', 'Ishibuchi, H.')\n",
      "('Scalable multi-objective optimization test problems', 'Deb, K.')\n",
      "('Enhanced decomposition-based hybrid evolutionary and gradient-based algorithm for many-objective optimization', 'Mohammad Zadeh, Parviz')\n",
      "\n",
      "\n",
      "./GA_papers/SPEA2.pdf\n",
      "('SPEA2: Improving the strength pareto evolutionary algorithm', 'Zitzler, Eckart')\n"
     ]
    }
   ],
   "source": [
    "### Preprocess pdf documents\n",
    "import pdfplumber\n",
    "import pdftotext\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "import importlib\n",
    "from utils_rag import *\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# modified from https://stackoverflow.com/questions/77045559/langchain-load-with-string\n",
    "def get_text_chunks_langchain(text, title, author):\n",
    "    \"\"\" Turns raw string into docs that conform with docs = loader.load()\"\"\"\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len\n",
    "    )\n",
    "    docs = [Document(page_content=x, metadata={\"title\":title, \"author\":author}) for x in text_splitter.split_text(text)]\n",
    "    return docs\n",
    "\n",
    "def load_pdf(filepath, bib_file, model, device, tokenizer):\n",
    "    \"\"\" From a pdf, return a docs\"\"\"\n",
    "    text = \"\"\n",
    "    match = get_title_author_from_pdf(filepath, bib_file, model, device, tokenizer)[0]\n",
    "    print(match)\n",
    "    title, author = match\n",
    "    with open(filepath, 'rb') as f:\n",
    "        pdf = pdftotext.PDF(f)\n",
    "        for page in pdf:\n",
    "            text += page\n",
    "    return get_text_chunks_langchain(text, title, author)\n",
    "\n",
    "# idea: train on malformed titles from bibliography to enhance similarity detection\n",
    "\n",
    "\"\"\" Unit test \"\"\"\n",
    "# pdf_files = [\"./GA_papers/CMA_ES.pdf\", \"./GA_papers/SBX.pdf\", './GA_papers/HypE.pdf', './GA_papers/SPEA2.pdf', './GA_papers/NSGA.pdf']\n",
    "directory = './GA_papers/'\n",
    "pattern = '*.pdf'\n",
    "pdf_files = glob.glob(os.path.join(directory, pattern))\n",
    "\n",
    "documents = []\n",
    "bib_file = \"./bib_data/paper_metadata_full.bib\"\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    docs = load_pdf(pdf_file, bib_file, bert_model, device, tokenizer)\n",
    "    documents += docs\n",
    "\n",
    "print('\\n')\n",
    "for pdf_file in pdf_files:\n",
    "    if pdf_file == \"./GA_papers/SPEA2.pdf\":\n",
    "        print(pdf_file)\n",
    "        docs = load_pdf(pdf_file, bib_file, bert_model, device, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8da143d",
   "metadata": {},
   "source": [
    "# 2. Preparation for Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "545b518c-6ffc-4d75-b9eb-99ba6e3f6e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5763fd72-f101-4774-9993-1ea0da2ba6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load model from local files\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_path = '/data1/model/bge1_5-large-zh'\n",
    "llm_path = '/data1/model/qwen1_5-7b-chat'\n",
    "\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "763c9bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence_transformers chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6458b360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "488be172-bb63-4281-a6c3-27367e716dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'author': 'Bartz-Beielstein, Thomas', 'title': 'Gradient-Based/Evolutionary Relay Hybrid for Computing Pareto Front Approximations Maximizing the S-Metric'}\n",
      "1995, pp. 1556‚Äì1561.\n",
      "[22] D. Van Veldhuizen, ‚ÄúMultiobjective evolutionary algorithms: Classifications, analyzes, and new innovations,‚Äù Air Force Inst. Technol., Dayton,\n",
      "OH, Tech. Rep. AFIT/DS/ENG/99-01, 1999.\n",
      "[23] D. Van Veldhuizen and G. Lamont, ‚ÄúMu\n",
      "{'author': 'Mohammad Zadeh, Parviz', 'title': 'Enhanced decomposition-based hybrid evolutionary and gradient-based algorithm for many-objective optimization'}\n",
      "[235] H. Esbensen, E.S. Kuh, Design space exploration using the genetic algorithm,\n",
      "in: IEEE Symposium on Circuits and Systems, ISCAS 1996, vol. 4, 1996, pp.\n",
      "500‚Äì503.\n",
      "[236] P. Czyzak, A. Jaszkiewicz, Pareto simulated annealing‚Äîa metaheuristic for\n",
      "mult\n",
      "{'author': 'Zitzler, Eckart', 'title': 'Comparison of Multiobjective Evolutionary Algorithms: Empirical Results'}\n",
      "Springer, Berlin, Germany.\n",
      "Zitzler, E. and Thiele, L. (1999). Multiobjective evolutionary algorithms: A comparative case study\n",
      "and the strength pareto approach. IEEE Transactions on Evolutionary Computation, 3(4):257‚Äì271.\n",
      "Evolutionary Computation\n",
      "Vol\n",
      "{'author': 'Beume, Nicola', 'title': 'SMS-EMOA: Multiobjective selection based on dominated hypervolume'}\n",
      "Parmee (Ed.), Adaptive Computing in Design and Manufacture VI (ACDM 2004), Springer, London, 2004, pp. 249‚Äì260.\n",
      "[27] M. Emmerich, B. Naujoks, Metamodel-assisted multi-objective optimisation with implicit constraints and their application in airfoil\n",
      "d\n"
     ]
    }
   ],
   "source": [
    "### Embed documents into vectordb\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, HuggingFaceBgeEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_path = '/data1/model/bge1_5-large-zh'\n",
    "# embeddings = SentenceTransformer(model_name_or_path=model_path, local_files_only=True)\n",
    "\n",
    "# current directory: /home/lzc/mindspore/ChatBabel.ipynb\n",
    "# embeddings = HuggingFaceBgeEmbeddings(model_name='BAAI/bge-large-zh-v1.5', cache_folder=model_path)\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name=model_path)#, cache_folder=model_path)\n",
    "\n",
    "vector_store = Chroma(embedding_function=embeddings)\n",
    "vector_store.add_documents(documents)\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "\"\"\" Unit test \"\"\"\n",
    "query = \"A crossover operator in the continuous space.\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "for doc in retrieved_docs:\n",
    "    print(doc.metadata)\n",
    "    print(doc.page_content[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b04fb4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'author': 'Bartz-Beielstein, Thomas', 'title': 'Gradient-Based/Evolutionary Relay Hybrid for Computing Pareto Front Approximations Maximizing the S-Metric'}\n",
      "1995, pp. 1556‚Äì1561.\n",
      "[22] D. Van Veldhuizen, ‚ÄúMultiobjective evolutionary algorithms: Classifications, analyzes, and new innovations,‚Äù Air Force Inst. Technol., Dayton,\n",
      "OH, Tech. Rep. AFIT/DS/ENG/99-01, 1999.\n",
      "[23] D. Van Veldhuizen and G. Lamont, ‚ÄúMu\n",
      "{'author': 'Mohammad Zadeh, Parviz', 'title': 'Enhanced decomposition-based hybrid evolutionary and gradient-based algorithm for many-objective optimization'}\n",
      "[235] H. Esbensen, E.S. Kuh, Design space exploration using the genetic algorithm,\n",
      "in: IEEE Symposium on Circuits and Systems, ISCAS 1996, vol. 4, 1996, pp.\n",
      "500‚Äì503.\n",
      "[236] P. Czyzak, A. Jaszkiewicz, Pareto simulated annealing‚Äîa metaheuristic for\n",
      "mult\n",
      "{'author': 'Zitzler, Eckart', 'title': 'Comparison of Multiobjective Evolutionary Algorithms: Empirical Results'}\n",
      "Springer, Berlin, Germany.\n",
      "Zitzler, E. and Thiele, L. (1999). Multiobjective evolutionary algorithms: A comparative case study\n",
      "and the strength pareto approach. IEEE Transactions on Evolutionary Computation, 3(4):257‚Äì271.\n",
      "Evolutionary Computation\n",
      "Vol\n",
      "{'author': 'Beume, Nicola', 'title': 'SMS-EMOA: Multiobjective selection based on dominated hypervolume'}\n",
      "Parmee (Ed.), Adaptive Computing in Design and Manufacture VI (ACDM 2004), Springer, London, 2004, pp. 249‚Äì260.\n",
      "[27] M. Emmerich, B. Naujoks, Metamodel-assisted multi-objective optimisation with implicit constraints and their application in airfoil\n",
      "d\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Unit test \"\"\"\n",
    "query = \"A crossover operator in the continuous space.\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "for doc in retrieved_docs:\n",
    "    print(doc.metadata)\n",
    "    print(doc.page_content[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a327db4-139f-4a8c-b179-92436c7f0377",
   "metadata": {},
   "source": [
    "# 3. Preparation for LLM module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f62b78e1-27d1-4a8d-b1d3-cd4199499435",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.751 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55a3598d1734dd29adb0b834cf8d894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MindSpore do not support bfloat16 dtype, we will automaticlly convert to float16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM<\n",
       "  (model): Qwen2Model<\n",
       "    (embed_tokens): Embedding<vocab_size=151936, embedding_size=4096, use_one_hot=False, weight=Parameter (Tensor(shape=[151936, 4096], dtype=Float16, value=[...], name=model.embed_tokens.weight), requires_grad=True), dtype=Float32, padding_idx=None>\n",
       "    (layers): CellList<\n",
       "      (0): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (1): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (2): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (3): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (4): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (5): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (6): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (7): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (8): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (9): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (10): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (11): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (12): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (13): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (14): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (15): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (16): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (17): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (18): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (19): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (20): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (21): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (22): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (23): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (24): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (25): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (26): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (27): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (28): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (29): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (30): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (31): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      >\n",
       "    (norm): Qwen2RMSNorm<>\n",
       "    >\n",
       "  (lm_head): Dense<input_channels=4096, output_channels=151936>\n",
       "  >"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mindspore as ms\n",
    "from mindnlp.transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "ms.context.set_context(device_target='GPU', device_id=2)\n",
    "llm_path = '/data1/model/qwen1_5-7b-chat'\n",
    "model = AutoModelForCausalLM.from_pretrained(llm_path)\n",
    "model.set_train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02edcb62-6375-4a00-a3e5-8f6c85618187",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(llm_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331d6cc4",
   "metadata": {},
   "source": [
    "# 4. Asking ChatBabel questions about research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37af1b5e-e8a4-4d35-b295-e5da41935ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzc/.conda/envs/huawei/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a2a635f4750451e9ae34c78fe061d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÊÇ®ÁöÑÁ†îÁ©∂ÊÉ≥Ê≥ï‰∏éDebÂú®\"Simulated Binary Crossover for Continuous Search Space\"Ôºà[12]Ôºâ‰∏≠Êé¢ËÆ®ÁöÑÂÜÖÂÆπÁõ∏‰ºº„ÄÇ‰ªñ‰ª¨Á°ÆÂÆûÊèêÂá∫‰∫Ü‰ΩøÁî®Ê®°Êãü‰∫åËøõÂà∂‰∫§ÂèâÔºàSimulated Binary Crossover, SBCÔºâÊù•Â§ÑÁêÜËøûÁª≠ÊêúÁ¥¢GCCÔºàÂç≥ËøûÁª≠ÂèòÈáèÔºâÔºåËøôÊòØ‰∏ÄÁßçÈÄöËøáÊ®°Êãü‰∫åËøõÂà∂ÁºñÁ†ÅÁöÑÊ¶ÇÂøµÔºåÂÆûÁé∞Âú®ËøûÁª≠Á©∫Èó¥‰∏≠ËøõË°å‰∫§ÂèâÊìç‰ΩúÁöÑÊñπÊ≥ï„ÄÇËøôÁßçÊñπÊ≥ïÈÄöËøáÊò†Â∞ÑÈóÆÈ¢òÂèòÈáèÂπ∂ÈÅøÂºÄ‰∫åËøõÂà∂ÁºñÁ†Å‰∏≠ÁöÑÂìàÂØÜÈ°øÊÇ¨Â¥ñÈóÆÈ¢òÊù•ÊèêÈ´ò‰∫§ÂèâÁöÑÁÅµÊ¥ªÊÄß„ÄÇ\n",
      "\n",
      "ÊÇ®ÊèêÂá∫ÁöÑÂú®ÂÆûÈôÖÊìç‰Ωú‰∏≠Ê†πÊçÆÊ¶ÇÁéáÂàÜÂ∏ÉÂª∫Ê®°ËøûÁª≠Á©∫Èó¥ÁöÑÂèòÂºÇÊìç‰ΩúÔºåÁ±ª‰ºº‰∫éSBC‰∏≠ÁöÑËøûÁª≠ÁâàÊú¨ÔºåÊòØ‰∏ÄÁßçÂêàÁêÜÁöÑÊºîÂåñÁÆóÊ≥ïÊîπËøõ„ÄÇÁêÜËÆ∫‰∏äÔºåËøôÊ†∑ÁöÑËÆæËÆ°ÂèØ‰ª•ÂáèÂ∞ë‰∫åËøõÂà∂ÁºñÁ†ÅÂ∏¶Êù•ÁöÑÈôêÂà∂ÔºåÂπ∂‰∏îÁêÜËÆ∫‰∏äÂàÜÊûê‰∫ÜÁ®ÄÁñèÊÄßÁöÑÂΩ±Âìç„ÄÇÂõ†Ê≠§ÔºåÊÇ®ÁöÑÊÉ≥Ê≥ïÊòØÂèØË°å‰∏îÂú®Â∑≤Áü•ÊñáÁåÆ‰∏≠ÊúâÊâÄÊé¢ËÆ®ÁöÑÔºå‰ΩÜÈúÄË¶ÅÂÖ∑‰ΩìÁ†îÁ©∂Êù•È™åËØÅÂÖ∂Âú®ÂÆûÈôÖ‰ºòÂåñÈóÆÈ¢ò‰∏≠ÁöÑÊïàÊûú„ÄÇ\n",
      "\n",
      "ÂèÇËÄÉËµÑÊñôÔºö\n",
      "{\"author\": \"Deb, Kalyanmoy\", \"title\": \"Simulated Binary Crossover for Continuous Search Space\"}\n",
      "{\"author\": \"Mohammad Zadeh, Parviz\", \"title\": \"Enhanced decomposition-based hybrid evolutionary and gradient-based algorithm for many-objective optimization\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mindspore import Tensor\n",
    "from mindspore import context\n",
    "from mindnlp.transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "import json\n",
    "\n",
    "def stream_generate_answer(\n",
    "    input_ids,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    max_new_tokens=300,\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.0,\n",
    "    context_len=2048\n",
    "):\n",
    "    streamer = TextIteratorStreamer(tokenizer, timeout=60.0, skip_prompt=True, skip_special_tokens=True)\n",
    "    max_src_len = context_len - max_new_tokens - 8\n",
    "    input_ids = input_ids[-max_src_len:]\n",
    "    \n",
    "    input_ids = Tensor(input_ids)\n",
    "    \n",
    "    generation_kwargs = dict(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        streamer=streamer,\n",
    "    )\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    yield from streamer\n",
    "\n",
    "def answer(prompt):\n",
    "\tcontext_str = \"\"\n",
    "\tretrieved_docs = retriever.get_relevant_documents(prompt)\n",
    "\tfor doc in retrieved_docs:\n",
    "\t\tcontext_str += json.dumps(doc.metadata)\n",
    "\t\tcontext_str += '\\n'\n",
    "\t\tcontext_str += doc.page_content[:100]\n",
    "\t\tcontext_str += '\\n'\n",
    "\n",
    "\tPROMPT_TEMPLATE = \"\"\"Âü∫‰∫é‰ª•‰∏ãÂ∑≤Áü•‰ø°ÊÅØÔºåÁÆÄÊ¥ÅÂíå‰∏ì‰∏öÁöÑÂëäÁü•Áî®Êà∑‰ªñ‰ª¨ÁöÑÁ†îÁ©∂ÊÉ≥Ê≥ïÊòØÂê¶Âá∫Áé∞Âú®Â∑≤Áü•‰ø°ÊÅØÁöÑÊñáÁåÆÁöÑÂÆûÈôÖÂÜÖÂÆπ‰∏≠„ÄÇ\n",
    "\tËØ∑Êèê‰æõÁõ∏ÂÖ≥Êù°ÁõÆÁöÑÊ†áÈ¢ò‰ª•Âèä‰ΩúËÄÖÔºå‰∏çÂÖÅËÆ∏Âú®Á≠îÊ°à‰∏≠Ê∑ªÂä†ÁºñÈÄ†ÊàêÂàÜÔºåÁ≠îÊ°àËØ∑‰ΩøÁî®‰∏≠Êñá„ÄÇ\n",
    "\n",
    "\tÂ∑≤Áü•‰ø°ÊÅØÔºö\n",
    "\t{context}\n",
    "\n",
    "\tËØ∑‰ªîÁªÜÊÄùËÄÉÂπ∂ÂõûÁ≠î„ÄÇ\n",
    "\t\"\"\".format(context=context_str)\n",
    "\n",
    "\tmessages = [\n",
    "\t\t{\"role\": \"system\", \"content\": PROMPT_TEMPLATE},\n",
    "\t\t{\"role\": \"user\", \"content\": prompt}\n",
    "\t]\n",
    "\n",
    "\tinput_ids = tokenizer.apply_chat_template(\n",
    "\t\tconversation=messages,\n",
    "\t\ttokenize=True,\n",
    "\t\tadd_generation_prompt=True,\n",
    "\t\treturn_tensors='ms'\n",
    "\t)\n",
    "\n",
    "\tresponse = \"\"\n",
    "\tfor new_text in stream_generate_answer(input_ids, tokenizer, model):\n",
    "\t\tresponse += new_text\n",
    "\tresponse = response.strip()\n",
    "\n",
    "\n",
    "\tunique_metadata = []\n",
    "\tfor retrieved_doc in retrieved_docs:\n",
    "\t\tmetadata = retrieved_doc.metadata\n",
    "\t\tif metadata['author'] in ('None', 'Unknown author') or metadata['title'] in ('None', 'Unknown title'):\n",
    "\t\t\tpass\n",
    "\t\telif metadata not in unique_metadata:\n",
    "\t\t\tunique_metadata.append(metadata)\n",
    "\n",
    "\treferences = \"\\nÂèÇËÄÉËµÑÊñôÔºö\\n\"\n",
    "\tfor item in unique_metadata:\n",
    "\t\treferences += json.dumps(item)\n",
    "\t\treferences += '\\n'\n",
    "\n",
    "\treturn response, references\n",
    "\n",
    "# prompt = \"I have a new idea! For a LLM, it's almost impossible to pre-train from scratch: too costly. A solution is to freeze all the parameters, and create a new low-rank estimation of the original weights and train those weights with reduced parameters. What do you think?\"\n",
    "prompt = \"\"\"\n",
    "I have a new idea! for evolutionary algorithms, we usually perform the crossover operation on discrete strings. \n",
    "but we could study the probability distribution of the variation operator and mathematically model them in a continuous space to\n",
    "perform a real-valued crossover operation. what do you think of this idea?\n",
    "\"\"\"\n",
    "\n",
    "# awesome answer.\n",
    "\n",
    "response, references = answer(prompt)\n",
    "print(response)\n",
    "print(references)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "617439a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d631270cb0d849148005ef5a6441b5aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÊÇ®ÁöÑÁ†îÁ©∂ÊÉ≥Ê≥ïÂú®Â∑≤Áü•ÊñáÁåÆ‰∏≠Ê≤°ÊúâÁõ¥Êé•ÊèêÂèä„ÄÇ\"Pre-training from scratch\"Âú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑËÆ≠ÁªÉ‰∏≠ÊòØ‰∏Ä‰∏™Â∏∏ËßÅÁöÑÁ≠ñÁï•ÔºåÂ∞§ÂÖ∂ÊòØÂú®TransformerÊû∂ÊûÑ‰∏≠ÔºåÈÄöËøáÂ§ßÈáèÊï∞ÊçÆËøõË°åÂàùÂßã.\",\"ÁÑ∂ËÄåÔºåÊÇ®ÊèêÂà∞ÁöÑ\"freeze all parameters\"ÁÑ∂Âêé\"create a new low-rank estimation and train with reduced parameters\"Ëøô‰∏™ÊñπÊ≥ïÊõ¥Êé•Ëøë‰∫éÊ®°ÂûãÂæÆË∞ÉÊàñËÄÖÊ®°ÂûãÂéãÁº©ÁöÑÈ¢ÜÂüüÔºåËÄåÈùû‰ªéÂ§¥ÂºÄÂßãÁöÑÈ¢ÑËÆ≠ÁªÉ„ÄÇËøô‰∏éIshibuchiÂú®\"Performance of Decomposition-Based Many-Objective Algorithms\"‰∏≠ËÆ®ËÆ∫ÁöÑÂèØËÉΩÈù¢ÂØπÁöÑ\"fitness evaluation mechanisms not always suitable for many-objective optimization\"ÔºàÂú®Â§öÁõÆÊ†á‰ºòÂåñ‰∏≠ÁöÑÈÄÇÂ∫îÊÄßÈóÆÈ¢òÔºâÊàñËÄÖBartz-BeielsteinÂú®\"Gradient-Based/Evolutionary Relay Hybrid\"‰∏≠ÊèêÂà∞ÁöÑÊ®°Âûã‰ºòÂåñÊäÄÊúØÊúâÁõ∏‰ºº‰πãÂ§ÑÔºå‰ΩÜÂÖ∑‰ΩìÊòØÂê¶ÂèØË°åÔºåÈúÄË¶ÅÁªìÂêàÂÆûÈôÖÁöÑÊï∞Â≠¶Ê®°ÂûãÂíåËØ≠Ë®ÄÊ®°ÂûãÁöÑÂ≠¶‰π†ÂéüÁêÜÊù•ÂàÜÊûê„ÄÇ\n",
      "\n",
      "Â¶ÇÊûúÊÇ®ÁöÑËß£ÂÜ≥ÊñπÊ°àÊòØÈíàÂØπÂ¶Ç‰ΩïÂú®ÊàêÊú¨ÊúâÈôêÁöÑÊÉÖÂÜµ‰∏ã‰ºòÂåñÊ®°ÂûãËÆ≠ÁªÉÔºåÂèØËÉΩÈúÄË¶ÅÁªìÂêàÊõ¥ÂÖ∑‰ΩìÁöÑ‰∏ä‰∏ãÊñáÊàñÁÆóÊ≥ïÁêÜËÆ∫Êù•ËÆ®ËÆ∫ÔºåËøôÂèØËÉΩÊõ¥Á¨¶ÂêàDebÁöÑ\"Niched-Penalty Approach for Constraint Handling\"‰∏≠Â§ÑÁêÜÁ∫¶ÊùüÂíåËµÑÊ∫êÊúâÈôêÈóÆÈ¢òÁöÑÊñπÊ≥ïËÆ∫„ÄÇ‰ΩÜ advocates for this exact technique in the context of LLM pre-training aren't covered in these references.\n",
      "\n",
      "Áªº‰∏äÔºåÊÇ®ÁöÑÊÉ≥Ê≥ïÂèØËÉΩÂú®Êüê‰∫õÁâπÂÆöÁöÑÊ®°Âûã‰ºòÂåñÊàñËµÑÊ∫êÁÆ°ÁêÜÁöÑÂú∫ÊôØ‰∏ãÊúâÂÖ∂ÂêàÁêÜÊÄßÔºå‰ΩÜË¶ÅÁ°ÆÂÆöÊòØÂê¶‰∏éÁé∞ÊúâÊñáÁåÆ‰∏≠ÁöÑÁ†îÁ©∂‰∏ÄËá¥ÔºåÈúÄË¶ÅËøõË°åËØ¶ÁªÜÁöÑÊñáÁåÆË∞ÉÁ†îÊàñÂÆûÈ™å\n",
      "\n",
      "ÂèÇËÄÉËµÑÊñôÔºö\n",
      "{\"author\": \"Ishibuchi, Hisao\", \"title\": \"Performance of Decomposition-Based Many-Objective Algorithms Strongly Depends on Pareto Front Shapes\"}\n",
      "{\"author\": \"Bartz-Beielstein, Thomas\", \"title\": \"Gradient-Based/Evolutionary Relay Hybrid for Computing Pareto Front Approximations Maximizing the S-Metric\"}\n",
      "{\"author\": \"Ishibuchi, H.\", \"title\": \"A multi-objective genetic local search algorithm and its application to flowshop scheduling\"}\n",
      "{\"author\": \"Deb, Kalyanmoy\", \"title\": \"A Niched-Penalty Approach for Constraint Handling in Genetic Algorithms\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"I have a new idea! For a LLM, it's almost impossible to pre-train from scratch: too costly. \n",
    "A solution is to freeze all the parameters, and create a new low-rank estimation of the original weights and train those weights with reduced parameters. \n",
    "What do you think?\"\"\"\n",
    "\n",
    "# No entry in database related to LoRA.\n",
    "\n",
    "response, references = answer(prompt)\n",
    "print(response)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e3a098e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f97a8f798b648798435cb2cf8fb8af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÊÇ®ÁöÑÁ†îÁ©∂ÊÉ≥Ê≥ïÊòØÂê¶Âá∫Áé∞Âú®Â∑≤Áü•‰ø°ÊÅØÁöÑÊñáÁåÆ‰∏≠Ôºü\n",
      "\n",
      "1. Zitzler, EckartÁöÑ\"Comparison of Multiobjective Evolutionary Algorithms: Empirical Results\"ÔºàÂú®\"Cybernetics\"ÊùÇÂøóÔºå1999Âπ¥ÔºåÁ¨¨28Âç∑ÔºåÁ¨¨1ÊúüÔºå38-÷™Õ®47È°µÔºâ‰∏≠ÊèêÂà∞‰∫ÜÂØπÊØîÂ§öÁõÆÊ†áËøõÂåñÁÆóÊ≥ïÁöÑÂÆûÈ™åÁªìÊûúÔºå‰ΩÜÂπ∂Ê≤°ÊúâÁõ¥Êé•ÊèêÂèäÂàõÂª∫Êñ∞ÈÅó‰º†ÁÆóÊ≥ïÁöÑÂÜÖÂÆπ„ÄÇÂ¶ÇÊûúÊÇ®ÊÉ≥ÊØîËæÉÊàñÊîπËøõÁé∞ÊúâÈÅó‰º†ÁÆóÊ≥ïÔºåËøôÂèØËÉΩÊòØ‰∏Ä‰∏™ÂèÇËÄÉÁÇπ„ÄÇ\n",
      "\n",
      "2. Fourman, M. P.Ôºà1985Âπ¥ÔºâÁöÑËÆ∫ÊñáËÆ®ËÆ∫‰∫ÜÈÅó‰º†ÁÆóÊ≥ïÁî®‰∫éÂ∏ÉÂ±ÄÂéãÁº©ÔºåËôΩÁÑ∂Ê≤°ÊúâÁõ¥Êé•ÂÖ≥‰∫éÊñ∞ÈÅó‰º†ÁÆóÊ≥ïÁöÑÂàõÈÄ†Ôºå‰ΩÜÂèØ‰ª•ÂêØÂèëÂØπÈÅó‰º†ÁÆóÊ≥ïÂ∫îÁî®ÁöÑÂàõÊñ∞ÊÄùËÄÉ„ÄÇ\n",
      "\n",
      "3. Bartz-Beielstein, ThomasÁöÑ‰∏§ÁØáËÆ∫ÊñáÔºå\"Gradient-Based/Evolutionary Relay Hybrid for Computing Pareto Front Approximations Maximizing the S-Metric\"ÔºåÂàÜÂà´Âú®2000Âπ¥Âíå2001Âπ¥ÂèëË°®ÔºåÊ∂âÂèä‰ΩøÁî®ÊüêÁßçÊ∑∑ÂêàÁÆóÊ≥ïÊù•‰ºòÂåñParetoÂâçÊ≤øÔºåËøôÂèØËÉΩ‰∏∫Êñ∞ÁÆóÊ≥ïËÆæËÆ°Êèê‰æõ‰∫ÜÁêÜËÆ∫Âü∫Á°Ä„ÄÇÂ¶ÇÊûúÊÇ®ÁöÑÊñ∞ÁÆóÊ≥ïÊòØËøôÁßçÊ∑∑ÂêàËøõÂåñÁ≠ñÁï•ÁöÑÂèò‰ΩìÔºåÈÇ£‰πàÂÆÉÂèØËÉΩ‰∏éËøô‰∫õÁ†îÁ©∂Êúâ KatieÔºàK.ÔºâÁöÑÂ∑•‰ΩúÁõ∏ÂÖ≥„ÄÇ\n",
      "\n",
      "4. Â¶ÇÊûúÊÇ®Âú®ËÆæËÆ°Êñ∞ÈÅó‰º†ÁÆóÊ≥ïÁöÑËøáÁ®ã‰∏≠Ôºå‰ΩøÁî®‰∫ÜÁ±ª‰ººËøõÂåñÊàñÊ¢ØÂ∫¶ÁöÑÊñπÊ≥ïÔºåÊàñËÄÖÁõÆÊ†áÊòØÊúÄÂ§ßÂåñÊüêÁßçËØÑ‰ª∑ÊåáÊ†áÔºàÂ¶ÇS-MetricÔºâÔºåÈÇ£‰πàËøô‰∫õËÆ∫ÊñáÂèØËÉΩ‰ºöÊèê‰æõÁÅµÊÑü„ÄÇ‰ΩÜË¶ÅÁ°ÆËÆ§\n",
      "\n",
      "ÂèÇËÄÉËµÑÊñôÔºö\n",
      "{\"author\": \"Zitzler, Eckart\", \"title\": \"Comparison of Multiobjective Evolutionary Algorithms: Empirical Results\"}\n",
      "{\"author\": \"Bartz-Beielstein, Thomas\", \"title\": \"Gradient-Based/Evolutionary Relay Hybrid for Computing Pareto Front Approximations Maximizing the S-Metric\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I would like to create a new genetic algorithm.\"\n",
    "\n",
    "# Wrong usage, but acceptable answer.\n",
    "\n",
    "response, references = answer(prompt)\n",
    "print(response)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b52542c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1aa6b5a179482481d3abf70a80d6cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÊÇ®ÁöÑÁ†îÁ©∂ÊÉ≥Ê≥ïÂú®Â∑≤Áü•‰ø°ÊÅØ‰∏≠ÊúâÊâÄ‰ΩìÁé∞„ÄÇ‰ΩúËÄÖKnowlesÂú®ËÆ∫Êñá\"The Pareto archived evolution strategy: a new baseline algorithm for Pareto multiobjective optimisation\"‰∏≠ÊèêÂà∞‰∫Ü‰∏ÄÁßçÂêç‰∏∫\"Pareto Archived Evolution Strategy (PAES)\"ÁöÑÁÆóÊ≥ïÔºåÂÆÉÂà©Áî®‰∫ÜÁ±ª‰ººÁöÑÊÄùÊÉ≥ÔºåÂç≥‰øùÁïôÂπ∂Êõ¥Êñ∞ÊúÄ‰ºòËß£‰ª•ÊîπËøõÂ§öÁõÆÊ†á‰ºòÂåñ„ÄÇÊÇ®ÁöÑÊèèËø∞Á±ª‰ºº‰∫éPAES‰∏≠ÁöÑ\"replacement mechanism\"ÔºåÂç≥Áî®archive‰∏≠ÁöÑÊúÄ‰Ω≥Ëß£ÂÜ≥ÊñπÊ°àÊõøÊç¢ÂΩìÂâçÁßçÁæ§‰∏≠ÁöÑËæÉÂ∑Æ‰∏™‰Ωì„ÄÇËøôÁßçÁ≠ñÁï•ÊúâÂä©‰∫éÁ°Æ‰øùËß£ÂÜ≥ÊñπÊ°àÁöÑÊîπËøõÔºåÂõ†‰∏∫ÂÆÉ‰øùÁïô‰∫ÜÁßçÁæ§‰∏≠ÁöÑÊúÄ‰ºòÂü∫Âõ†„ÄÇÂõ†Ê≠§ÔºåÊÇ®ÁöÑÁ†îÁ©∂ÊÉ≥Ê≥ïÊòØPAESÁÆóÊ≥ïÁöÑ‰∏ÄÁßçÂèò‰ΩìÊàñÁõ∏ÂÖ≥Ê¶ÇÂøµÔºåÂ∑≤ÁªèÂú®ÊñáÁåÆ‰∏≠Ë¢´ËÆ®ËÆ∫Ëøá„ÄÇ\n",
      "\n",
      "ÂèÇËÄÉËµÑÊñôÔºö\n",
      "{\"author\": \"Knowles, J.\", \"title\": \"The Pareto archived evolution strategy: a new baseline algorithm for Pareto multiobjective optimisation\"}\n",
      "{\"author\": \"Bartz-Beielstein, Thomas\", \"title\": \"Gradient-Based/Evolutionary Relay Hybrid for Computing Pareto Front Approximations Maximizing the S-Metric\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"I have come up with a new genetic algorithm that utilizes the concept of Pareto dominance. \n",
    "What if we store all the best solutions so far in an archive, and replace the worst solutions in the current population with the best solutions in that archive?\n",
    "Then, each generation will only preserve the genes from the best individuals, and the solution is surely guaranteed to improve!\n",
    "\"\"\"\n",
    "\n",
    "# PAES is actually a very good catch: I didn't remember what this algorithm was when I asked this question.\n",
    "\n",
    "response, references = answer(prompt)\n",
    "print(response)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bdad47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0adf36a72e4c498cb45a6ed787bdd848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÊòØÁöÑÔºåÊàëÁÜüÊÇâhypervolumeËøô‰∏™Ê¶ÇÂøµ„ÄÇHypervolumeÊòØ‰∏ÄÁßçÂ§öÁõÆÊ†á‰ºòÂåñ‰∏≠ÁöÑËØÑ‰ª∑ÊåáÊ†áÔºåÂÆÉË°°Èáè‰∏ÄÁªÑËß£Áõ∏ÂØπ‰∫éÂèÇËÄÉÁÇπÁöÑÊÄßËÉΩÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§öÁõÆÊ†á‰ºòÂåñÁÆóÊ≥ï‰∏≠Áî®‰∫éËØÑ‰º∞ÂíåplotlibÂÜ≥Á≠ñÂèòÈáè„ÄÇÁî±‰∫éÂÆÉÊòØ‰∏Ä‰∏™Ê†áÈáèÔºàÂç≥Âçï‰∏™ÂÄºÔºâÔºåÁêÜËÆ∫‰∏äÂèØ‰ª•ÂØπÂÖ∂ÂÅèÂØºÊï∞ËøõË°åËÆ°ÁÆóÔºå‰ª•ÊåáÂØº‰ºòÂåñËøáÁ®ã„ÄÇ\n",
      "\n",
      "ÁêÜËÆ∫‰∏äÔºåÈÄöËøáËÆ°ÁÆóhypervolumeÂØπÂÜ≥Á≠ñÂèòÈáèÁöÑÂÅèÂØºheldÔºåÊàë‰ª¨ÂèØ‰ª•ÊâæÂà∞ÈÇ£‰∫õËÉΩÊòæËëóÊèêÈ´òhypervolumeÁöÑÊúÄ‰ºòÊñπÂêëÔºå‰ªéËÄåÂú®Êüê‰∫õ‰ºòÂåñÁÆóÊ≥ï‰∏≠ÔºåÂ¶ÇHypEÔºàBader, 2020ÔºâÁ≠âÔºåÂèØ‰ª•Âà©Áî®Ê¢ØÂ∫¶‰ø°ÊÅØÊù•Ëø≠‰ª£Êõ¥Êñ∞Ëß£Ôºå‰ª•‰øÉËøõÂêëÊõ¥Â•ΩÁöÑhypervolumeÂå∫ÂüüÊî∂Êïõ„ÄÇÁÑ∂ËÄåÔºåÂÆûÈôÖÊìç‰Ωú‰∏≠ÔºåÁî±‰∫éhypervolumeÁöÑËÆ°ÁÆóÊ∂âÂèäÂà∞Â§ö‰∏™ÁõÆÊ†áÂíåÂèÇËÄÉÁÇπÔºåÁõ¥Êé•Ê±ÇÂÅèÂØºÂπ∂‰∏çÁõ¥ËßÇÔºåÈÄöÂ∏∏ÈúÄË¶Å‰ΩøÁî®Êï∞ÂÄº‰ºòÂåñÊñπÊ≥ïÊù•Ëøë‰ººÊ±ÇËß£„ÄÇ\n",
      "\n",
      "ÂèÇËÄÉËµÑÊñôÔºö\n",
      "{\"author\": \"Auger, Anne\", \"title\": \"Theory of the hypervolume indicator: optimal \\u03bc-distributions and the choice of the reference point\"}\n",
      "{\"author\": \"Ishibuchi, H.\", \"title\": \"A multi-objective genetic local search algorithm and its application to flowshop scheduling\"}\n",
      "{\"author\": \"Bader, Johannes\", \"title\": \"HypE: An Algorithm for Fast Hypervolume-Based Many-Objective Optimization\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Are you familiar with the concept of hypervolume? Since it is a metric (therefore scalar), \n",
    "can we take its derivative with respect to the decision variables? Then, we can nudge the solutions toward better hypervolume, therefore guaranteeing convergence.\n",
    "\"\"\"\n",
    "\n",
    "# HypE is fine, but I was thinking of HIGA-MO.\n",
    "\n",
    "response, references = answer(prompt)\n",
    "print(response)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bde3d95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61fa55ed2b7f49e8acb35e13116605aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÊÇ®ÁöÑÁ†îÁ©∂ÊÉ≥Ê≥ïÂú®Â∑≤Áü•ÊñáÁåÆ‰∏≠ÊúâÊâÄ‰ΩìÁé∞„ÄÇ‰ΩúËÄÖBartz-BeielsteinÂú®\"Gradient-Based/Evolutionary Relay Hybrid for Computing Pareto Front Approximations Maximizing the S-Metric\"‰∏≠ÊèêÂà∞‰∫Ü‰ΩøÁî®ÈÅó‰º†ÁÆóÊ≥ïÂ§ÑÁêÜÂ§öÁõÆÊ†áÈóÆÈ¢òÁöÑ‰∏ÄÁßçÊñπÊ≥ïÔºåÂÖ∂‰∏≠ÊèêÂà∞Â∞ÜÂ§öÁõÆÊ†áÈóÆÈ¢òÂàÜËß£‰∏∫ÂçïÁõÆÊ†áÂ≠êÈóÆÈ¢òÂπ∂Â§ÑÁêÜ„ÄÇËøôÁßçÊñπÊ≥ïÁ±ª‰ºº‰∫éÊÇ®ÊèèËø∞ÁöÑÈÄöËøáÂàÜËß£ÁöÑËß£ÂÜ≥ÊñπÊ°àÁßçÁæ§ËøõË°å‰∫§ÂèâÊìç‰ΩúÁöÑÊÄùË∑Ø„ÄÇÂ∞ΩÁÆ°Ê≤°ÊúâÁõ¥Êé•ÊèêÂà∞‰ΩøÁî®\"Â§ö‰∏™Á≤æËã±Ëß£ÂÜ≥ÊñπÊ°à\"Ôºàmultiple elite solutionsÔºâÊõø‰ª£Âçï‰∏ÄÁ≤æËã±Ëß£ÂÜ≥ÊñπÊ°àÔºå‰ΩÜËøôÁßçÂ§ÑÁêÜÁ≠ñÁï•ÊòØÈÅó‰º†ÁÆóÊ≥ï‰∏≠Â∏∏ËßÅÁöÑÂ§öÁõÆÊ†á‰ºòÂåñÁ≠ñÁï•ÔºåÈÄöËøá‰øùÁïôÂíåÊìç‰ΩúÂ§ö‰∏™Â•ΩÁöÑËß£Êù•ÂØªÊâæÈùûÂä£Ëß£„ÄÇÂõ†Ê≠§ÔºåÊÇ®ÁöÑÊÉ≥Ê≥ïÂú®ÊüêÁßçÁ®ãÂ∫¶‰∏ä‰∏éÊüê‰∫õÂ§öÁõÆÊ†áÈÅó‰º†ÁÆóÊ≥ïÁöÑÂÆûË∑µÁõ∏ÂêªÂêà„ÄÇ\n",
      "\n",
      "ÂèÇËÄÉËµÑÊñôÔºö\n",
      "{\"author\": \"Zitzler, Eckart\", \"title\": \"SPEA2: Improving the strength pareto evolutionary algorithm\"}\n",
      "{\"author\": \"Bartz-Beielstein, Thomas\", \"title\": \"Gradient-Based/Evolutionary Relay Hybrid for Computing Pareto Front Approximations Maximizing the S-Metric\"}\n",
      "{\"author\": \"Sinha, Ankur\", \"title\": \"A Review on Bilevel Optimization: From Classical to Evolutionary Approaches and Applications\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"What if we try to decompose multi-objective problems into single objective problems and solve them instead? \n",
    "Specifically, I'm thinking of using a population of decomposed solutions and performing crossover on them. \n",
    "Have there been precedents in multiobjective genetic algorithms that do this?\n",
    "\"\"\"\n",
    "\n",
    "# this one is kinda wrong, supposed to be MOEA/D\n",
    "\n",
    "response, references = answer(prompt)\n",
    "print(response)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8ab3b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f7ae927e10848a4ae0af2137557b0ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÂÖ≥‰∫éÊºîÂåñÁÆóÊ≥ïÁöÑÊúÄÊó©ÁöÑÊñáÁåÆÊòØYen, J.Y. (1971) Finding the K Shortest Loopless PathsÔºå‰ΩúËÄÖÊòØJ.Y. Yen„ÄÇ\n",
      "\n",
      "ÂèÇËÄÉËµÑÊñôÔºö\n",
      "{\"author\": \"Knowles, J.\", \"title\": \"The Pareto archived evolution strategy: a new baseline algorithm for Pareto multiobjective optimisation\"}\n",
      "{\"author\": \"Zitzler, Eckart\", \"title\": \"Comparison of Multiobjective Evolutionary Algorithms: Empirical Results\"}\n",
      "{\"author\": \"Bartz-Beielstein, Thomas\", \"title\": \"Gradient-Based/Evolutionary Relay Hybrid for Computing Pareto Front Approximations Maximizing the S-Metric\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"ÊàëÈóÆ‰Ω†Ôºå‰Ω†ÁöÑÊï∞ÊçÆÂ∫ìÈáåÂÖ≥‰∫éÊºîÂåñÁÆóÊ≥ïËÉΩÊü•Âà∞ÊúÄÊó©ÁöÑÊñáÁåÆÊòØÂì™ÁØáÔºü\n",
    "\"\"\"\n",
    "\n",
    "# wrong usage as well, should pose a research idea instead of asking questions like LLM\n",
    "# this is wrong, supposed to be VEGA 1985 or No free lunch theorem (sometime around 1970?)\n",
    "\n",
    "response, references = answer(prompt)\n",
    "print(response)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9796ee0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb98fe19cb89400bb31824f99cf1af2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÊòØÁöÑÔºåhypervolume metricÂú®Â§öÁõÆÊ†á‰ºòÂåñÔºàMany-Objective Optimization, MMOÔºâ‰∏≠Ë¢´ÂπøÊ≥õ‰ΩøÁî®ÔºåÁâπÂà´ÊòØÂú®ËØÑ‰º∞ÂíåÈÄâÊã©Ëß£ÂÜ≥ÊñπÊ°àÊó∂„ÄÇHypervolumeÊòØ‰∏ÄÁßçË°°ÈáèÈõÜÂêàÊÄßËÉΩÁöÑÂÖ®Â±ÄÊåáÊ†áÔºåÂÆÉËÄÉËôë‰∫ÜÊâÄÊúâÁõÆÊ†áÂáΩÊï∞ÁöÑÂÄºÔºåËÄå‰∏ç‰ªÖ‰ªÖÊòØÂçï‰∏™‰∏™‰ΩìÁöÑfitness„ÄÇHypEÔºàHypervolume-based Evolutionary Algorithm, BaderÁ≠â‰∫∫Âú®2015Âπ¥ÁöÑËÆ∫Êñá‰∏≠ÊèêÂá∫ÔºâÂ∞±ÊòØ‰∏Ä‰∏™‰ΩøÁî®ËøôÁßçÊåáÊ†áÁöÑÁÆóÊ≥ïÂÆû‰æã„ÄÇ\n",
      "\n",
      "HypEÁÆóÊ≥ï‰∏ç‰ªÖÈÄÇÁî®‰∫éÂõ∫ÂÆöÊï∞ÈáèÁöÑÂÆ¢ËßÇÂáΩÊï∞ÔºàÂ¶Ç4Êàñ5ÔºâÔºåÂÆÉËÉΩÂ§üÂ§ÑÁêÜ‰ªªÊÑèÊï∞ÈáèÁöÑÂ§öÁõÆÊ†áÈóÆÈ¢ò„ÄÇÂú®ÁÆóÊ≥ï‰∏≠ÔºåHypervolume-based Fitness AssignmentË¢´Áî®Êù•ËØÑ‰º∞ÂíåÈÄâÊã©‰∏™‰ΩìÔºåËÄå‰∏çÊòØ‰º†ÁªüÁöÑ –º–µ—Ç—Ä–æÔºàËæπÈôÖÊîπËøõÔºâÊàñÂç†Áî®Â∫¶ÔºàdominanceÔºâËøôÊ†∑ÁöÑÂ±ÄÈÉ®ÊåáÊ†á„ÄÇËøôÊ†∑ÂÅöÁöÑÂ•ΩÂ§ÑÊòØÔºåÂç≥‰ΩøÂú®Â§öÁõÆÊ†á‰ºòÂåñ‰∏≠ÔºåHypervolumeÂèØ‰ª•Êèê‰æõ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÊÄßËÉΩËßÜËßíÔºåÊúâÂä©‰∫éÊâæÂà∞Âú®ÊâÄÊúâÁõÆÊ†á‰∏äÈÉΩË°®Áé∞ËâØÂ•ΩÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ\n",
      "\n",
      "Âõ†Ê≠§Ôºå‰Ω†ÁöÑÊÉ≥Ê≥ïÊòØÂêàÁêÜÁöÑÔºåËÆ∏Â§öÁ†îÁ©∂Á°ÆÂÆûÈááÁî®HypervolumeÊù•Êõø‰ª£‰º†ÁªüÁöÑfitness functionÔºåÁâπÂà´ÊòØÂú®ÈúÄË¶ÅÂÖ®Â±Ä‰ºòÂåñÁöÑÂú∫ÊôØ‰∏≠„ÄÇ\n",
      "\n",
      "ÂèÇËÄÉËµÑÊñôÔºö\n",
      "{\"author\": \"Bader, Johannes\", \"title\": \"HypE: An Algorithm for Fast Hypervolume-Based Many-Objective Optimization\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"‰Ω†ÂØπhypervolume metric‰∫ÜËß£ÂêóÔºüÊúâÊ≤°ÊúâÊºîÂåñÁÆóÊ≥ïÁé∞Âú®Áî®hypervolume metric‰Ωú‰∏∫ÊåáÊ†áÔºå‰ª£ÊõøÊàë‰ª¨‰πãÂâçÊâÄÁî®ÁöÑfitness functionÊù•ËØÑÂà§‰∏Ä‰∏™‰∏™‰ΩìÈÄÇ‰∏çÈÄÇÂêàÁîüÂ≠òÔºüÊàëËßâÂæóËøôÊòØ‰∏™ÂæàÂ•ΩÁöÑ‰∏ªÊÑè„ÄÇ\n",
    "\"\"\"\n",
    "\n",
    "# looking for SMS-EMOA, but HypE is acceptable\n",
    "\n",
    "response, references = answer(prompt)\n",
    "print(response)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d573a12b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fb6d6595adc41a28bedaa231395f102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÊÇ®ÁöÑÁ†îÁ©∂ÊÉ≥Ê≥ï‰∏éDeb, K.ÁöÑ\"Scalable multi-objective optimization test problems\"Áõ∏Á¨¶Âêà„ÄÇÂú®ËØ•ÊñáÁåÆ‰∏≠Ôºå‰ΩúËÄÖËÆ®ËÆ∫‰∫ÜÂåÖÊã¨DTLZ5ÂíåDTLZ7Âú®ÂÜÖÁöÑÂ§öÁõÆÊ†á‰ºòÂåñÈóÆÈ¢òÔºåËøô‰∫õÈóÆÈ¢òÈÄöÂ∏∏Âú®È´òÁª¥Á©∫Èó¥‰∏≠ËÆæËÆ°ÔºåÊó®Âú®Êèê‰æõÊµãËØïÂ§öÁõÆÊ†á‰ºòÂåñÁÆóÊ≥ïÁöÑÂü∫ÂáÜÈóÆÈ¢ò„ÄÇÊÇ®ÂèØ‰ª•ÂÄüÈâ¥Ëøô‰∫õÂ∑≤ÊúâÁöÑÈóÆÈ¢òÊù•ÊûÑÂª∫ÊÇ®ÁöÑÊñ∞Âü∫ÂáÜÈõÜ„ÄÇ\n",
      "\n",
      "ÂèÇËÄÉËµÑÊñôÔºö\n",
      "{\"author\": \"Deb, K.\", \"title\": \"Scalable multi-objective optimization test problems\"}\n",
      "{\"author\": \"Mohammad Zadeh, Parviz\", \"title\": \"Enhanced decomposition-based hybrid evolutionary and gradient-based algorithm for many-objective optimization\"}\n",
      "{\"author\": \"Bader, Johannes\", \"title\": \"HypE: An Algorithm for Fast Hypervolume-Based Many-Objective Optimization\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "ÊàëÊÉ≥Ë¶ÅËÆæËÆ°‰∏Ä‰∫õÊñ∞ÁöÑÈÄÇÁî®‰∫éÂ§öÁõÆÊ†á‰ºòÂåñÈóÆÈ¢òÁöÑbenchmark problem setÔºåÂèØËÉΩÈÄÇÁî®‰∫éÈ´òÁª¥Á©∫Èó¥ÁöÑÈóÆÈ¢ò„ÄÇ\n",
    "\"\"\"\n",
    "\n",
    "# DTLZ/WFG are both fine\n",
    "\n",
    "response, references = answer(prompt)\n",
    "print(response)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ddfa84f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d26b7d89d9e4448a9f31e80da5e2afc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÊÇ®ÁöÑÁ†îÁ©∂ÊÉ≥Ê≥ïÂèØËÉΩ‰∏é\"HypE: An Algorithm for Fast Hypervolume-Based Many-Objective Optimization\"ÔºàBader, JohannesÔºâ‰∏≠ÁöÑÊüê‰∫õÂÜÖÂÆπÁõ∏ÂÖ≥ÔºåÁâπÂà´ÊòØÂú®Â§ÑÁêÜÂ§öÁõÆÊ†á‰ºòÂåñÂíåÈÄÇÂ∫îÊÄßÂàÜÂ∏ÉÊó∂ÔºåÁÆóÊ≥ïÂèØËÉΩÊ∂âÂèäÂà∞ËøõÂåñÁ≠ñÁï•„ÄÇÁÑ∂ËÄåÔºåËØ•ÊñáÁåÆ‰∏ªË¶ÅÂÖ≥Ê≥®ÁöÑÊòØÂü∫‰∫éhypervolumeÁöÑÂ§öÁõÆÊ†á‰ºòÂåñÁÆóÊ≥ïÔºåËÄå‰∏çÊòØÁõ¥Êé•Â§ÑÁêÜÊ¶ÇÁéáÊ®°ÂûãÊàñGaussianÂàÜÂ∏ÉÁöÑÊºîÂåñ„ÄÇÂ¶ÇÊûúÊÇ®ÊÉ≥Êé¢ËÆ®GaussianÂàÜÂ∏ÉÁöÑÈÄÇÂ∫îÊÄßÂª∫Ê®°ÔºåÂèØËÉΩÈúÄË¶ÅÊü•ÈòÖËøõÂåñËÆ°ÁÆó„ÄÅÊú∫Âô®Â≠¶‰π†ÊàñÁªüËÆ°Âª∫Ê®°ÁöÑÊñáÁåÆÔºåÊØîÂ¶Ç‰ΩøÁî®ÈÅó‰º†ÁÆóÊ≥ïÊàñÁ≤íÂ≠êÁæ§‰ºòÂåñÁ≠âÊñπÊ≥ï‰ºòÂåñÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞„ÄÇ\n",
      "\n",
      "\"SMS-EMOA: Multiobjective selection based on dominated hypervolume\"ÔºàBeume, NicolaÔºâ‰∏≠ÊèêÂà∞ÁöÑÂ§öÁõÆÊ†áÈÄâÊã©Á≠ñÁï•ÔºåËôΩÁÑ∂‰∏çÊòØÁõ¥Êé•Â§ÑÁêÜGaussiansÔºå‰ΩÜÂ§öÁõÆÊ†á‰ºòÂåñÊñπÊ≥ïÂèØËÉΩ‰ºöÁî®Âà∞ÈÄÇÂ∫îÊÄßÂàÜÂ∏ÉÔºåËøô‰∏éÊÇ®ÁöÑÊÉ≥Ê≥ïÊúâÂÖ≥„ÄÇ\n",
      "\n",
      "\"Laumanns, M.\"ÁöÑ\"Áªü‰∏ÄÁöÑÂ§öÁõÆÊ†áËøõÂåñÁÆóÊ≥ïÊ®°Âûã\"ÔºàA unified model for multi-objective evolutionary algorithms with elitismÔºâÂèØËÉΩÊ∂âÂèäÈÄÇÂ∫îÊÄßÁ≠ñÁï•ÁöÑÂàùÂßãÂåñÔºåËøôÂú®‰ºòÂåñÊ®°ÂûãÂèÇÊï∞Êó∂‰πüÂèØËÉΩÈÄÇÁî®Ôºå‰ΩÜÂÖ∑‰ΩìÂà∞GaussiansÁöÑÊºîÂåñÔºåÈúÄË¶ÅËøõ‰∏ÄÊ≠•ÂàÜÊûê„ÄÇ\n",
      "\n",
      "\"Auger, AnneÁöÑ'Hypervolume Indicator'ÁêÜËÆ∫\"ËÆ®ËÆ∫‰∫ÜÈÄâÊã©ÂèÇËÄÉÁÇπÂíå‰ºòÂåñÊåáÊ†áÔºåËøôÂú®ÊûÑÂª∫ÂíåËØÑ‰º∞Ê¶ÇÁéáÊ®°ÂûãÊó∂ÂèØËÉΩÈó¥Êé•Áõ∏ÂÖ≥Ôºå‰ΩÜ‰∏çÊòØÁõ¥Êé•ÁöÑGaussianÂàÜÂ∏ÉÊºîÂåñ„ÄÇ\n",
      "\n",
      "Áªº‰∏äÔºåÊÇ®ÁöÑÊÉ≥Ê≥ïÂèØËÉΩ‰∏éÈÉ®ÂàÜÊñáÁåÆÁöÑÂ§öÁõÆÊ†á‰ºòÂåñÊàñÈÄÇÂ∫îÊÄßÂª∫\n",
      "\n",
      "ÂèÇËÄÉËµÑÊñôÔºö\n",
      "{\"author\": \"Bader, Johannes\", \"title\": \"HypE: An Algorithm for Fast Hypervolume-Based Many-Objective Optimization\"}\n",
      "{\"author\": \"Beume, Nicola\", \"title\": \"SMS-EMOA: Multiobjective selection based on dominated hypervolume\"}\n",
      "{\"author\": \"Laumanns, M.\", \"title\": \"A unified model for multi-objective evolutionary algorithms with elitism\"}\n",
      "{\"author\": \"Auger, Anne\", \"title\": \"Theory of the hypervolume indicator: optimal \\u03bc-distributions and the choice of the reference point\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Ok, so I have this idea of \"evolving\" Gaussian distributions to fit a probabilistic model of some intrinsic dataset.\n",
    "\"\"\"\n",
    "\n",
    "# It tried, it should've replied with CMA-ES or MO-CMA-ES, but I guess the context length was too short.\n",
    "\n",
    "response, references = answer(prompt)\n",
    "print(response)\n",
    "print(references)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huawei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
