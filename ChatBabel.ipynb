{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3c04d64",
   "metadata": {},
   "source": [
    "### current directory: /home/lzc/mindspore/ChatBabel.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b34820db-8267-4795-9037-c6a9d8273fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/data1/model/bge1_5-large-zh'\n",
    "llm_path = '/data1/model/qwen1_5-7b-chat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f37e253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun  4 06:59:07 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 6000 Ada Gene...    Off |   00000000:3B:00.0 Off |                  Off |\n",
      "| 30%   30C    P8             26W /  300W |   38975MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX 6000 Ada Gene...    Off |   00000000:5E:00.0 Off |                  Off |\n",
      "| 30%   30C    P8             27W /  300W |   26397MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA RTX 6000 Ada Gene...    Off |   00000000:B1:00.0 Off |                  Off |\n",
      "| 30%   29C    P8             21W /  300W |       4MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA RTX 6000 Ada Gene...    Off |   00000000:D9:00.0 Off |                  Off |\n",
      "| 30%   30C    P8             28W /  300W |    1039MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   1962785      C   ...zc/.conda/envs/workspace/bin/python      38006MiB |\n",
      "|    0   N/A  N/A   2788981      C   ...e/lzc/.conda/envs/huawei/bin/python        958MiB |\n",
      "|    1   N/A  N/A   1899540      C   /home/ah/.conda/envs/ah/bin/python          26390MiB |\n",
      "|    3   N/A  N/A   2788981      C   ...e/lzc/.conda/envs/huawei/bin/python       1032MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c794505",
   "metadata": {},
   "source": [
    "## Preparing papers\n",
    "1. Locate the zip file that contains the papers and unzip them into the `./data` repository.\n",
    "2. Manually create a .bib file that contains all the metadata for the papers and store them in `./bib_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9915ec49-c7d2-419b-b5d4-8b3613c02f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture captured_output\n",
    "# !unzip papers_condensed.zip -d ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b88db04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotDict(dict):\n",
    "    def __getattr__(self, key):\n",
    "        try:\n",
    "            return self[key]\n",
    "        except KeyError:\n",
    "            raise AttributeError(f\"'DotDict' object has no attribute '{key}'\")\n",
    "\n",
    "    def __setattr__(self, key, value):\n",
    "        self[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a750c9-94b8-48e2-ac0f-4181bbc1b9ac",
   "metadata": {},
   "source": [
    "# 1. Preparation for PDF loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d10f3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://hf-mirror.com/bert-base-uncased/resolve/main/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "from multitask_classifier import *\n",
    "from utils import *\n",
    "from utils_rag import *\n",
    "from tokenizer import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "config = {'hidden_dropout_prob': 0.3,\n",
    "\t\t\t'num_labels': {3: 0, 4: 1, 2: 2, 1: 3, 0: 4},\n",
    "\t\t\t'hidden_size': 768,\n",
    "\t\t\t'data_dir': '.',\n",
    "\t\t\t'option': 'finetune'}\n",
    "\n",
    "config = SimpleNamespace(**config)\n",
    "\n",
    "argpath = './finetune-5-1e-05-multitask-final-2.pt'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:2')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "saved = torch.load(argpath)\n",
    "\n",
    "bert_model = MultitaskBERT(config)\n",
    "add_lora_layers(bert_model)\n",
    "for name, param in bert_model.named_parameters():\n",
    "\tif \"lora\" not in name and \"classifier\" not in name and \"bias\" not in name:\n",
    "\t\tparam.requires_grad = False\n",
    "\telse:\n",
    "\t\tparam.requires_grad = True\n",
    "bert_model.load_state_dict(saved['model'])\n",
    "bert_model = bert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b044c390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.9694808125495911\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"gradient-based/evolutionary relay hybrid for computing pareto front approximations maximizing the s-metric\"\n",
    "sentence2 = \"\"\"\n",
    "muiltiobj ective optimization using nondominated sorting in genetic algorithms\n",
    "\"\"\"\n",
    "\n",
    "print(detect_paraphrase(bert_model, device, tokenizer, sentence1, sentence2))\n",
    "print(detect_similarity(bert_model, device, tokenizer, sentence1, sentence2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7ad1b1d-bbd1-4410-a7d6-92c6541419de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A Survey on Evolutionary Computation for Computer Vision and Image Analysis: Past, Present, and Future Trends', 'Bi, Ying')\n",
      "('Modified Distance Calculation in Generational Distance and Inverted Generational Distance', 'Gaspar-Cunha, António')\n",
      "('Gradient-Based/Evolutionary Relay Hybrid for Computing Pareto Front Approximations Maximizing the S-Metric', 'Bartz-Beielstein, Thomas')\n",
      "('The Pareto archived evolution strategy: a new baseline algorithm for Pareto multiobjective optimisation', 'Knowles, J.')\n",
      "('Enhanced decomposition-based hybrid evolutionary and gradient-based algorithm for many-objective optimization', 'Mohammad Zadeh, Parviz')\n",
      "('Computing Hypervolume Contributions in Low Dimensions: Asymptotically Optimal Algorithm and Complexity Results', 'Takahashi, Ricardo H. C.')\n",
      "('A Scalable Multi-objective Test Problem Toolkit', 'Coello Coello, Carlos A.')\n",
      "('Enhanced decomposition-based hybrid evolutionary and gradient-based algorithm for many-objective optimization', 'Mohammad Zadeh, Parviz')\n",
      "('Performance of Decomposition-Based Many-Objective Algorithms Strongly Depends on Pareto Front Shapes', 'Ishibuchi, Hisao')\n",
      "('The CMA Evolution Strategy: A Tutorial', 'Hansen, Nikolaus')\n",
      "('MOEA/D: A Multiobjective Evolutionary Algorithm Based on Decomposition', '{Qingfu Zhang}')\n",
      "('Gradient-Based/Evolutionary Relay Hybrid for Computing Pareto Front Approximations Maximizing the S-Metric', 'Bartz-Beielstein, Thomas')\n",
      "('Comparison of Multiobjective Evolutionary Algorithms: Empirical Results', 'Zitzler, Eckart')\n",
      "('A parallel global multiobjective framework for optimization: pagmo', 'Biscani, Francesco')\n",
      "('SPEA2: Improving the strength pareto evolutionary algorithm', 'Zitzler, Eckart')\n",
      "('Muiltiobjective Optimization Using Nondominated Sorting in Genetic Algorithms', 'Srinivas, N.')\n",
      "('A Review on Bilevel Optimization: From Classical to Evolutionary Approaches and Applications', 'Sinha, Ankur')\n",
      "('A multi-objective genetic local search algorithm and its application to flowshop scheduling', 'Ishibuchi, H.')\n",
      "('A Niched-Penalty Approach for Constraint Handling in Genetic Algorithms', 'Deb, Kalyanmoy')\n",
      "('SMS-EMOA: Multiobjective selection based on dominated hypervolume', 'Beume, Nicola')\n",
      "('Gradient-Based/Evolutionary Relay Hybrid for Computing Pareto Front Approximations Maximizing the S-Metric', 'Bartz-Beielstein, Thomas')\n",
      "('Theory of the hypervolume indicator: optimal μ-distributions and the choice of the reference point', 'Auger, Anne')\n",
      "('HypE: An Algorithm for Fast Hypervolume-Based Many-Objective Optimization', 'Bader, Johannes')\n",
      "('Unknown title', 'Unknown author')\n",
      "('Multiobjective evolutionary algorithms: a comparative case study and the strength Pareto approach', 'Zitzler, E.')\n",
      "('Simulated Binary Crossover for Continuous Search Space', 'Deb, Kalyanmoy')\n",
      "('A unified model for multi-objective evolutionary algorithms with elitism', 'Laumanns, M.')\n",
      "('Gradient-Based/Evolutionary Relay Hybrid for Computing Pareto Front Approximations Maximizing the S-Metric', 'Bartz-Beielstein, Thomas')\n",
      "('A multi-objective genetic local search algorithm and its application to flowshop scheduling', 'Ishibuchi, H.')\n",
      "('Scalable multi-objective optimization test problems', 'Deb, K.')\n",
      "('Enhanced decomposition-based hybrid evolutionary and gradient-based algorithm for many-objective optimization', 'Mohammad Zadeh, Parviz')\n",
      "\n",
      "\n",
      "./GA_papers/SPEA2.pdf\n",
      "('SPEA2: Improving the strength pareto evolutionary algorithm', 'Zitzler, Eckart')\n"
     ]
    }
   ],
   "source": [
    "### Preprocess pdf documents\n",
    "import pdfplumber\n",
    "import pdftotext\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "import importlib\n",
    "from utils_rag import *\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# modified from https://stackoverflow.com/questions/77045559/langchain-load-with-string\n",
    "def get_text_chunks_langchain(text, title, author):\n",
    "    \"\"\" Turns raw string into docs that conform with docs = loader.load()\"\"\"\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len\n",
    "    )\n",
    "    docs = [Document(page_content=x, metadata={\"title\":title, \"author\":author}) for x in text_splitter.split_text(text)]\n",
    "    return docs\n",
    "\n",
    "def load_pdf(filepath, bib_file, model, device, tokenizer):\n",
    "    \"\"\" From a pdf, return a docs\"\"\"\n",
    "    text = \"\"\n",
    "    match = get_title_author_from_pdf(filepath, bib_file, model, device, tokenizer)[0]\n",
    "    print(match)\n",
    "    title, author = match\n",
    "    with open(filepath, 'rb') as f:\n",
    "        pdf = pdftotext.PDF(f)\n",
    "        for page in pdf:\n",
    "            text += page\n",
    "    return get_text_chunks_langchain(text, title, author)\n",
    "\n",
    "# idea: train on malformed titles from bibliography to enhance similarity detection\n",
    "\n",
    "\"\"\" Unit test \"\"\"\n",
    "# pdf_files = [\"./GA_papers/CMA_ES.pdf\", \"./GA_papers/SBX.pdf\", './GA_papers/HypE.pdf', './GA_papers/SPEA2.pdf', './GA_papers/NSGA.pdf']\n",
    "directory = './GA_papers/'\n",
    "pattern = '*.pdf'\n",
    "pdf_files = glob.glob(os.path.join(directory, pattern))\n",
    "\n",
    "documents = []\n",
    "bib_file = \"./bib_data/paper_metadata_full.bib\"\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    docs = load_pdf(pdf_file, bib_file, bert_model, device, tokenizer)\n",
    "    documents += docs\n",
    "\n",
    "print('\\n')\n",
    "for pdf_file in pdf_files:\n",
    "    if pdf_file == \"./GA_papers/SPEA2.pdf\":\n",
    "        print(pdf_file)\n",
    "        docs = load_pdf(pdf_file, bib_file, bert_model, device, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8da143d",
   "metadata": {},
   "source": [
    "# 2. Preparation for Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "545b518c-6ffc-4d75-b9eb-99ba6e3f6e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5763fd72-f101-4774-9993-1ea0da2ba6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load model from local files\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_path = '/data1/model/bge1_5-large-zh'\n",
    "llm_path = '/data1/model/qwen1_5-7b-chat'\n",
    "\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "763c9bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence_transformers chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6458b360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "488be172-bb63-4281-a6c3-27367e716dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'author': 'Bartz-Beielstein, Thomas', 'title': 'Gradient-Based/Evolutionary Relay Hybrid for Computing Pareto Front Approximations Maximizing the S-Metric'}\n",
      "1995, pp. 1556–1561.\n",
      "[22] D. Van Veldhuizen, “Multiobjective evolutionary algorithms: Classifications, analyzes, and new innovations,” Air Force Inst. Technol., Dayton,\n",
      "OH, Tech. Rep. AFIT/DS/ENG/99-01, 1999.\n",
      "[23] D. Van Veldhuizen and G. Lamont, “Mu\n",
      "{'author': 'Mohammad Zadeh, Parviz', 'title': 'Enhanced decomposition-based hybrid evolutionary and gradient-based algorithm for many-objective optimization'}\n",
      "[235] H. Esbensen, E.S. Kuh, Design space exploration using the genetic algorithm,\n",
      "in: IEEE Symposium on Circuits and Systems, ISCAS 1996, vol. 4, 1996, pp.\n",
      "500–503.\n",
      "[236] P. Czyzak, A. Jaszkiewicz, Pareto simulated annealing—a metaheuristic for\n",
      "mult\n",
      "{'author': 'Zitzler, Eckart', 'title': 'Comparison of Multiobjective Evolutionary Algorithms: Empirical Results'}\n",
      "Springer, Berlin, Germany.\n",
      "Zitzler, E. and Thiele, L. (1999). Multiobjective evolutionary algorithms: A comparative case study\n",
      "and the strength pareto approach. IEEE Transactions on Evolutionary Computation, 3(4):257–271.\n",
      "Evolutionary Computation\n",
      "Vol\n",
      "{'author': 'Beume, Nicola', 'title': 'SMS-EMOA: Multiobjective selection based on dominated hypervolume'}\n",
      "Parmee (Ed.), Adaptive Computing in Design and Manufacture VI (ACDM 2004), Springer, London, 2004, pp. 249–260.\n",
      "[27] M. Emmerich, B. Naujoks, Metamodel-assisted multi-objective optimisation with implicit constraints and their application in airfoil\n",
      "d\n"
     ]
    }
   ],
   "source": [
    "### Embed documents into vectordb\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, HuggingFaceBgeEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_path = '/data1/model/bge1_5-large-zh'\n",
    "# embeddings = SentenceTransformer(model_name_or_path=model_path, local_files_only=True)\n",
    "\n",
    "# current directory: /home/lzc/mindspore/ChatBabel.ipynb\n",
    "# embeddings = HuggingFaceBgeEmbeddings(model_name='BAAI/bge-large-zh-v1.5', cache_folder=model_path)\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name=model_path)#, cache_folder=model_path)\n",
    "\n",
    "vector_store = Chroma(embedding_function=embeddings)\n",
    "vector_store.add_documents(documents)\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "\"\"\" Unit test \"\"\"\n",
    "query = \"A crossover operator in the continuous space.\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "for doc in retrieved_docs:\n",
    "    print(doc.metadata)\n",
    "    print(doc.page_content[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b04fb4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'author': 'Bartz-Beielstein, Thomas', 'title': 'Gradient-Based/Evolutionary Relay Hybrid for Computing Pareto Front Approximations Maximizing the S-Metric'}\n",
      "1995, pp. 1556–1561.\n",
      "[22] D. Van Veldhuizen, “Multiobjective evolutionary algorithms: Classifications, analyzes, and new innovations,” Air Force Inst. Technol., Dayton,\n",
      "OH, Tech. Rep. AFIT/DS/ENG/99-01, 1999.\n",
      "[23] D. Van Veldhuizen and G. Lamont, “Mu\n",
      "{'author': 'Mohammad Zadeh, Parviz', 'title': 'Enhanced decomposition-based hybrid evolutionary and gradient-based algorithm for many-objective optimization'}\n",
      "[235] H. Esbensen, E.S. Kuh, Design space exploration using the genetic algorithm,\n",
      "in: IEEE Symposium on Circuits and Systems, ISCAS 1996, vol. 4, 1996, pp.\n",
      "500–503.\n",
      "[236] P. Czyzak, A. Jaszkiewicz, Pareto simulated annealing—a metaheuristic for\n",
      "mult\n",
      "{'author': 'Zitzler, Eckart', 'title': 'Comparison of Multiobjective Evolutionary Algorithms: Empirical Results'}\n",
      "Springer, Berlin, Germany.\n",
      "Zitzler, E. and Thiele, L. (1999). Multiobjective evolutionary algorithms: A comparative case study\n",
      "and the strength pareto approach. IEEE Transactions on Evolutionary Computation, 3(4):257–271.\n",
      "Evolutionary Computation\n",
      "Vol\n",
      "{'author': 'Beume, Nicola', 'title': 'SMS-EMOA: Multiobjective selection based on dominated hypervolume'}\n",
      "Parmee (Ed.), Adaptive Computing in Design and Manufacture VI (ACDM 2004), Springer, London, 2004, pp. 249–260.\n",
      "[27] M. Emmerich, B. Naujoks, Metamodel-assisted multi-objective optimisation with implicit constraints and their application in airfoil\n",
      "d\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Unit test \"\"\"\n",
    "query = \"A crossover operator in the continuous space.\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "for doc in retrieved_docs:\n",
    "    print(doc.metadata)\n",
    "    print(doc.page_content[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a327db4-139f-4a8c-b179-92436c7f0377",
   "metadata": {},
   "source": [
    "# 3. Preparation for LLM module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f62b78e1-27d1-4a8d-b1d3-cd4199499435",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.751 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55a3598d1734dd29adb0b834cf8d894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MindSpore do not support bfloat16 dtype, we will automaticlly convert to float16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM<\n",
       "  (model): Qwen2Model<\n",
       "    (embed_tokens): Embedding<vocab_size=151936, embedding_size=4096, use_one_hot=False, weight=Parameter (Tensor(shape=[151936, 4096], dtype=Float16, value=[...], name=model.embed_tokens.weight), requires_grad=True), dtype=Float32, padding_idx=None>\n",
       "    (layers): CellList<\n",
       "      (0): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (1): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (2): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (3): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (4): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (5): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (6): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (7): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (8): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (9): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (10): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (11): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (12): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (13): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (14): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (15): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (16): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (17): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (18): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (19): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (20): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (21): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (22): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (23): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (24): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (25): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (26): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (27): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (28): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (29): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (30): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      (31): Qwen2DecoderLayer<\n",
       "        (self_attn): Qwen2Attention<\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096, has_bias=True>\n",
       "          (o_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (rotary_emb): Qwen2RotaryEmbedding<>\n",
       "          >\n",
       "        (mlp): Qwen2MLP<\n",
       "          (gate_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (up_proj): Dense<input_channels=4096, output_channels=11008>\n",
       "          (down_proj): Dense<input_channels=11008, output_channels=4096>\n",
       "          (act_fn): SiLU<>\n",
       "          >\n",
       "        (input_layernorm): Qwen2RMSNorm<>\n",
       "        (post_attention_layernorm): Qwen2RMSNorm<>\n",
       "        >\n",
       "      >\n",
       "    (norm): Qwen2RMSNorm<>\n",
       "    >\n",
       "  (lm_head): Dense<input_channels=4096, output_channels=151936>\n",
       "  >"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mindspore as ms\n",
    "from mindnlp.transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "ms.context.set_context(device_target='GPU', device_id=2)\n",
    "llm_path = '/data1/model/qwen1_5-7b-chat'\n",
    "model = AutoModelForCausalLM.from_pretrained(llm_path)\n",
    "model.set_train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02edcb62-6375-4a00-a3e5-8f6c85618187",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(llm_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331d6cc4",
   "metadata": {},
   "source": [
    "# 4. Asking ChatBabel questions about research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37af1b5e-e8a4-4d35-b295-e5da41935ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzc/.conda/envs/huawei/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a2a635f4750451e9ae34c78fe061d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您的研究想法与Deb在\"Simulated Binary Crossover for Continuous Search Space\"（[12]）中探讨的内容相似。他们确实提出了使用模拟二进制交叉（Simulated Binary Crossover, SBC）来处理连续搜索GCC（即连续变量），这是一种通过模拟二进制编码的概念，实现在连续空间中进行交叉操作的方法。这种方法通过映射问题变量并避开二进制编码中的哈密顿悬崖问题来提高交叉的灵活性。\n",
      "\n",
      "您提出的在实际操作中根据概率分布建模连续空间的变异操作，类似于SBC中的连续版本，是一种合理的演化算法改进。理论上，这样的设计可以减少二进制编码带来的限制，并且理论上分析了稀疏性的影响。因此，您的想法是可行且在已知文献中有所探讨的，但需要具体研究来验证其在实际优化问题中的效果。\n",
      "\n",
      "参考资料：\n",
      "{\"author\": \"Deb, Kalyanmoy\", \"title\": \"Simulated Binary Crossover for Continuous Search Space\"}\n",
      "{\"author\": \"Mohammad Zadeh, Parviz\", \"title\": \"Enhanced decomposition-based hybrid evolutionary and gradient-based algorithm for many-objective optimization\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mindspore import Tensor\n",
    "from mindspore import context\n",
    "from mindnlp.transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "import json\n",
    "\n",
    "def stream_generate_answer(\n",
    "    input_ids,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    max_new_tokens=300,\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.0,\n",
    "    context_len=2048\n",
    "):\n",
    "    streamer = TextIteratorStreamer(tokenizer, timeout=60.0, skip_prompt=True, skip_special_tokens=True)\n",
    "    max_src_len = context_len - max_new_tokens - 8\n",
    "    input_ids = input_ids[-max_src_len:]\n",
    "    \n",
    "    input_ids = Tensor(input_ids)\n",
    "    \n",
    "    generation_kwargs = dict(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        streamer=streamer,\n",
    "    )\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    yield from streamer\n",
    "\n",
    "def answer(prompt):\n",
    "\tcontext_str = \"\"\n",
    "\tretrieved_docs = retriever.get_relevant_documents(prompt)\n",
    "\tfor doc in retrieved_docs:\n",
    "\t\tcontext_str += json.dumps(doc.metadata)\n",
    "\t\tcontext_str += '\\n'\n",
    "\t\tcontext_str += doc.page_content[:100]\n",
    "\t\tcontext_str += '\\n'\n",
    "\n",
    "\tPROMPT_TEMPLATE = \"\"\"基于以下已知信息，简洁和专业的告知用户他们的研究想法是否出现在已知信息的文献的实际内容中。\n",
    "\t请提供相关条目的标题以及作者，不允许在答案中添加编造成分，答案请使用中文。\n",
    "\n",
    "\t已知信息：\n",
    "\t{context}\n",
    "\n",
    "\t请仔细思考并回答。\n",
    "\t\"\"\".format(context=context_str)\n",
    "\n",
    "\tmessages = [\n",
    "\t\t{\"role\": \"system\", \"content\": PROMPT_TEMPLATE},\n",
    "\t\t{\"role\": \"user\", \"content\": prompt}\n",
    "\t]\n",
    "\n",
    "\tinput_ids = tokenizer.apply_chat_template(\n",
    "\t\tconversation=messages,\n",
    "\t\ttokenize=True,\n",
    "\t\tadd_generation_prompt=True,\n",
    "\t\treturn_tensors='ms'\n",
    "\t)\n",
    "\n",
    "\tresponse = \"\"\n",
    "\tfor new_text in stream_generate_answer(input_ids, tokenizer, model):\n",
    "\t\tresponse += new_text\n",
    "\tresponse = response.strip()\n",
    "\n",
    "\n",
    "\tunique_metadata = []\n",
    "\tfor retrieved_doc in retrieved_docs:\n",
    "\t\tmetadata = retrieved_doc.metadata\n",
    "\t\tif metadata['author'] in ('None', 'Unknown author') or metadata['title'] in ('None', 'Unknown title'):\n",
    "\t\t\tpass\n",
    "\t\telif metadata not in unique_metadata:\n",
    "\t\t\tunique_metadata.append(metadata)\n",
    "\n",
    "\treferences = \"\\n参考资料：\\n\"\n",
    "\tfor item in unique_metadata:\n",
    "\t\treferences += json.dumps(item)\n",
    "\t\treferences += '\\n'\n",
    "\n",
    "\treturn response, references\n",
    "\n",
    "# prompt = \"I have a new idea! For a LLM, it's almost impossible to pre-train from scratch: too costly. A solution is to freeze all the parameters, and create a new low-rank estimation of the original weights and train those weights with reduced parameters. What do you think?\"\n",
    "prompt = \"\"\"\n",
    "I have a new idea! for evolutionary algorithms, we usually perform the crossover operation on discrete strings. \n",
    "but we could study the probability distribution of the variation operator and mathematically model them in a continuous space to\n",
    "perform a real-valued crossover operation. what do you think of this idea?\n",
    "\"\"\"\n",
    "\n",
    "# awesome answer.\n",
    "\n",
    "response, references = answer(prompt)\n",
    "print(response)\n",
    "print(references)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "617439a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d631270cb0d849148005ef5a6441b5aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您的研究想法在已知文献中没有直接提及。\"Pre-training from scratch\"在大型语言模型（LLM）的训练中是一个常见的策略，尤其是在Transformer架构中，通过大量数据进行初始.\",\"然而，您提到的\"freeze all parameters\"然后\"create a new low-rank estimation and train with reduced parameters\"这个方法更接近于模型微调或者模型压缩的领域，而非从头开始的预训练。这与Ishibuchi在\"Performance of Decomposition-Based Many-Objective Algorithms\"中讨论的可能面对的\"fitness evaluation mechanisms not always suitable for many-objective optimization\"（在多目标优化中的适应性问题）或者Bartz-Beielstein在\"Gradient-Based/Evolutionary Relay Hybrid\"中提到的模型优化技术有相似之处，但具体是否可行，需要结合实际的数学模型和语言模型的学习原理来分析。\n",
      "\n",
      "如果您的解决方案是针对如何在成本有限的情况下优化模型训练，可能需要结合更具体的上下文或算法理论来讨论，这可能更符合Deb的\"Niched-Penalty Approach for Constraint Handling\"中处理约束和资源有限问题的方法论。但 advocates for this exact technique in the context of LLM pre-training aren't covered in these references.\n",
      "\n",
      "综上，您的想法可能在某些特定的模型优化或资源管理的场景下有其合理性，但要确定是否与现有文献中的研究一致，需要进行详细的文献调研或实验\n",
      "\n",
      "参考资料：\n",
      "{\"author\": \"Ishibuchi, Hisao\", \"title\": \"Performance of Decomposition-Based Many-Objective Algorithms Strongly Depends on Pareto Front Shapes\"}\n",
      "{\"author\": \"Bartz-Beielstein, Thomas\", \"title\": \"Gradient-Based/Evolutionary Relay Hybrid for Computing Pareto Front Approximations Maximizing the S-Metric\"}\n",
      "{\"author\": \"Ishibuchi, H.\", \"title\": \"A multi-objective genetic local search algorithm and its application to flowshop scheduling\"}\n",
      "{\"author\": \"Deb, Kalyanmoy\", \"title\": \"A Niched-Penalty Approach for Constraint Handling in Genetic Algorithms\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"I have a new idea! For a LLM, it's almost impossible to pre-train from scratch: too costly. \n",
    "A solution is to freeze all the parameters, and create a new low-rank estimation of the original weights and train those weights with reduced parameters. \n",
    "What do you think?\"\"\"\n",
    "\n",
    "# No entry in database related to LoRA.\n",
    "\n",
    "response, references = answer(prompt)\n",
    "print(response)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e3a098e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f97a8f798b648798435cb2cf8fb8af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您的研究想法是否出现在已知信息的文献中？\n",
      "\n",
      "1. Zitzler, Eckart的\"Comparison of Multiobjective Evolutionary Algorithms: Empirical Results\"（在\"Cybernetics\"杂志，1999年，第28卷，第1期，38-֪ͨ47页）中提到了对比多目标进化算法的实验结果，但并没有直接提及创建新遗传算法的内容。如果您想比较或改进现有遗传算法，这可能是一个参考点。\n",
      "\n",
      "2. Fourman, M. P.（1985年）的论文讨论了遗传算法用于布局压缩，虽然没有直接关于新遗传算法的创造，但可以启发对遗传算法应用的创新思考。\n",
      "\n",
      "3. Bartz-Beielstein, Thomas的两篇论文，\"Gradient-Based/Evolutionary Relay Hybrid for Computing Pareto Front Approximations Maximizing the S-Metric\"，分别在2000年和2001年发表，涉及使用某种混合算法来优化Pareto前沿，这可能为新算法设计提供了理论基础。如果您的新算法是这种混合进化策略的变体，那么它可能与这些研究有 Katie（K.）的工作相关。\n",
      "\n",
      "4. 如果您在设计新遗传算法的过程中，使用了类似进化或梯度的方法，或者目标是最大化某种评价指标（如S-Metric），那么这些论文可能会提供灵感。但要确认\n",
      "\n",
      "参考资料：\n",
      "{\"author\": \"Zitzler, Eckart\", \"title\": \"Comparison of Multiobjective Evolutionary Algorithms: Empirical Results\"}\n",
      "{\"author\": \"Bartz-Beielstein, Thomas\", \"title\": \"Gradient-Based/Evolutionary Relay Hybrid for Computing Pareto Front Approximations Maximizing the S-Metric\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I would like to create a new genetic algorithm.\"\n",
    "\n",
    "# Wrong usage, but acceptable answer.\n",
    "\n",
    "response, references = answer(prompt)\n",
    "print(response)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b52542c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1aa6b5a179482481d3abf70a80d6cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您的研究想法在已知信息中有所体现。作者Knowles在论文\"The Pareto archived evolution strategy: a new baseline algorithm for Pareto multiobjective optimisation\"中提到了一种名为\"Pareto Archived Evolution Strategy (PAES)\"的算法，它利用了类似的思想，即保留并更新最优解以改进多目标优化。您的描述类似于PAES中的\"replacement mechanism\"，即用archive中的最佳解决方案替换当前种群中的较差个体。这种策略有助于确保解决方案的改进，因为它保留了种群中的最优基因。因此，您的研究想法是PAES算法的一种变体或相关概念，已经在文献中被讨论过。\n",
      "\n",
      "参考资料：\n",
      "{\"author\": \"Knowles, J.\", \"title\": \"The Pareto archived evolution strategy: a new baseline algorithm for Pareto multiobjective optimisation\"}\n",
      "{\"author\": \"Bartz-Beielstein, Thomas\", \"title\": \"Gradient-Based/Evolutionary Relay Hybrid for Computing Pareto Front Approximations Maximizing the S-Metric\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"I have come up with a new genetic algorithm that utilizes the concept of Pareto dominance. \n",
    "What if we store all the best solutions so far in an archive, and replace the worst solutions in the current population with the best solutions in that archive?\n",
    "Then, each generation will only preserve the genes from the best individuals, and the solution is surely guaranteed to improve!\n",
    "\"\"\"\n",
    "\n",
    "# PAES is actually a very good catch: I didn't remember what this algorithm was when I asked this question.\n",
    "\n",
    "response, references = answer(prompt)\n",
    "print(response)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bdad47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0adf36a72e4c498cb45a6ed787bdd848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "是的，我熟悉hypervolume这个概念。Hypervolume是一种多目标优化中的评价指标，它衡量一组解相对于参考点的性能，尤其是在多目标优化算法中用于评估和plotlib决策变量。由于它是一个标量（即单个值），理论上可以对其偏导数进行计算，以指导优化过程。\n",
      "\n",
      "理论上，通过计算hypervolume对决策变量的偏导held，我们可以找到那些能显著提高hypervolume的最优方向，从而在某些优化算法中，如HypE（Bader, 2020）等，可以利用梯度信息来迭代更新解，以促进向更好的hypervolume区域收敛。然而，实际操作中，由于hypervolume的计算涉及到多个目标和参考点，直接求偏导并不直观，通常需要使用数值优化方法来近似求解。\n",
      "\n",
      "参考资料：\n",
      "{\"author\": \"Auger, Anne\", \"title\": \"Theory of the hypervolume indicator: optimal \\u03bc-distributions and the choice of the reference point\"}\n",
      "{\"author\": \"Ishibuchi, H.\", \"title\": \"A multi-objective genetic local search algorithm and its application to flowshop scheduling\"}\n",
      "{\"author\": \"Bader, Johannes\", \"title\": \"HypE: An Algorithm for Fast Hypervolume-Based Many-Objective Optimization\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Are you familiar with the concept of hypervolume? Since it is a metric (therefore scalar), \n",
    "can we take its derivative with respect to the decision variables? Then, we can nudge the solutions toward better hypervolume, therefore guaranteeing convergence.\n",
    "\"\"\"\n",
    "\n",
    "# HypE is fine, but I was thinking of HIGA-MO.\n",
    "\n",
    "response, references = answer(prompt)\n",
    "print(response)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bde3d95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61fa55ed2b7f49e8acb35e13116605aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您的研究想法在已知文献中有所体现。作者Bartz-Beielstein在\"Gradient-Based/Evolutionary Relay Hybrid for Computing Pareto Front Approximations Maximizing the S-Metric\"中提到了使用遗传算法处理多目标问题的一种方法，其中提到将多目标问题分解为单目标子问题并处理。这种方法类似于您描述的通过分解的解决方案种群进行交叉操作的思路。尽管没有直接提到使用\"多个精英解决方案\"（multiple elite solutions）替代单一精英解决方案，但这种处理策略是遗传算法中常见的多目标优化策略，通过保留和操作多个好的解来寻找非劣解。因此，您的想法在某种程度上与某些多目标遗传算法的实践相吻合。\n",
      "\n",
      "参考资料：\n",
      "{\"author\": \"Zitzler, Eckart\", \"title\": \"SPEA2: Improving the strength pareto evolutionary algorithm\"}\n",
      "{\"author\": \"Bartz-Beielstein, Thomas\", \"title\": \"Gradient-Based/Evolutionary Relay Hybrid for Computing Pareto Front Approximations Maximizing the S-Metric\"}\n",
      "{\"author\": \"Sinha, Ankur\", \"title\": \"A Review on Bilevel Optimization: From Classical to Evolutionary Approaches and Applications\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"What if we try to decompose multi-objective problems into single objective problems and solve them instead? \n",
    "Specifically, I'm thinking of using a population of decomposed solutions and performing crossover on them. \n",
    "Have there been precedents in multiobjective genetic algorithms that do this?\n",
    "\"\"\"\n",
    "\n",
    "# this one is kinda wrong, supposed to be MOEA/D\n",
    "\n",
    "response, references = answer(prompt)\n",
    "print(response)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8ab3b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f7ae927e10848a4ae0af2137557b0ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "关于演化算法的最早的文献是Yen, J.Y. (1971) Finding the K Shortest Loopless Paths，作者是J.Y. Yen。\n",
      "\n",
      "参考资料：\n",
      "{\"author\": \"Knowles, J.\", \"title\": \"The Pareto archived evolution strategy: a new baseline algorithm for Pareto multiobjective optimisation\"}\n",
      "{\"author\": \"Zitzler, Eckart\", \"title\": \"Comparison of Multiobjective Evolutionary Algorithms: Empirical Results\"}\n",
      "{\"author\": \"Bartz-Beielstein, Thomas\", \"title\": \"Gradient-Based/Evolutionary Relay Hybrid for Computing Pareto Front Approximations Maximizing the S-Metric\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"我问你，你的数据库里关于演化算法能查到最早的文献是哪篇？\n",
    "\"\"\"\n",
    "\n",
    "# wrong usage as well, should pose a research idea instead of asking questions like LLM\n",
    "# this is wrong, supposed to be VEGA 1985 or No free lunch theorem (sometime around 1970?)\n",
    "\n",
    "response, references = answer(prompt)\n",
    "print(response)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9796ee0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb98fe19cb89400bb31824f99cf1af2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "是的，hypervolume metric在多目标优化（Many-Objective Optimization, MMO）中被广泛使用，特别是在评估和选择解决方案时。Hypervolume是一种衡量集合性能的全局指标，它考虑了所有目标函数的值，而不仅仅是单个个体的fitness。HypE（Hypervolume-based Evolutionary Algorithm, Bader等人在2015年的论文中提出）就是一个使用这种指标的算法实例。\n",
      "\n",
      "HypE算法不仅适用于固定数量的客观函数（如4或5），它能够处理任意数量的多目标问题。在算法中，Hypervolume-based Fitness Assignment被用来评估和选择个体，而不是传统的 метро（边际改进）或占用度（dominance）这样的局部指标。这样做的好处是，即使在多目标优化中，Hypervolume可以提供一个全面的性能视角，有助于找到在所有目标上都表现良好的解决方案。\n",
      "\n",
      "因此，你的想法是合理的，许多研究确实采用Hypervolume来替代传统的fitness function，特别是在需要全局优化的场景中。\n",
      "\n",
      "参考资料：\n",
      "{\"author\": \"Bader, Johannes\", \"title\": \"HypE: An Algorithm for Fast Hypervolume-Based Many-Objective Optimization\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"你对hypervolume metric了解吗？有没有演化算法现在用hypervolume metric作为指标，代替我们之前所用的fitness function来评判一个个体适不适合生存？我觉得这是个很好的主意。\n",
    "\"\"\"\n",
    "\n",
    "# looking for SMS-EMOA, but HypE is acceptable\n",
    "\n",
    "response, references = answer(prompt)\n",
    "print(response)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d573a12b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fb6d6595adc41a28bedaa231395f102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您的研究想法与Deb, K.的\"Scalable multi-objective optimization test problems\"相符合。在该文献中，作者讨论了包括DTLZ5和DTLZ7在内的多目标优化问题，这些问题通常在高维空间中设计，旨在提供测试多目标优化算法的基准问题。您可以借鉴这些已有的问题来构建您的新基准集。\n",
      "\n",
      "参考资料：\n",
      "{\"author\": \"Deb, K.\", \"title\": \"Scalable multi-objective optimization test problems\"}\n",
      "{\"author\": \"Mohammad Zadeh, Parviz\", \"title\": \"Enhanced decomposition-based hybrid evolutionary and gradient-based algorithm for many-objective optimization\"}\n",
      "{\"author\": \"Bader, Johannes\", \"title\": \"HypE: An Algorithm for Fast Hypervolume-Based Many-Objective Optimization\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "我想要设计一些新的适用于多目标优化问题的benchmark problem set，可能适用于高维空间的问题。\n",
    "\"\"\"\n",
    "\n",
    "# DTLZ/WFG are both fine\n",
    "\n",
    "response, references = answer(prompt)\n",
    "print(response)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ddfa84f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d26b7d89d9e4448a9f31e80da5e2afc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您的研究想法可能与\"HypE: An Algorithm for Fast Hypervolume-Based Many-Objective Optimization\"（Bader, Johannes）中的某些内容相关，特别是在处理多目标优化和适应性分布时，算法可能涉及到进化策略。然而，该文献主要关注的是基于hypervolume的多目标优化算法，而不是直接处理概率模型或Gaussian分布的演化。如果您想探讨Gaussian分布的适应性建模，可能需要查阅进化计算、机器学习或统计建模的文献，比如使用遗传算法或粒子群优化等方法优化概率密度函数。\n",
      "\n",
      "\"SMS-EMOA: Multiobjective selection based on dominated hypervolume\"（Beume, Nicola）中提到的多目标选择策略，虽然不是直接处理Gaussians，但多目标优化方法可能会用到适应性分布，这与您的想法有关。\n",
      "\n",
      "\"Laumanns, M.\"的\"统一的多目标进化算法模型\"（A unified model for multi-objective evolutionary algorithms with elitism）可能涉及适应性策略的初始化，这在优化模型参数时也可能适用，但具体到Gaussians的演化，需要进一步分析。\n",
      "\n",
      "\"Auger, Anne的'Hypervolume Indicator'理论\"讨论了选择参考点和优化指标，这在构建和评估概率模型时可能间接相关，但不是直接的Gaussian分布演化。\n",
      "\n",
      "综上，您的想法可能与部分文献的多目标优化或适应性建\n",
      "\n",
      "参考资料：\n",
      "{\"author\": \"Bader, Johannes\", \"title\": \"HypE: An Algorithm for Fast Hypervolume-Based Many-Objective Optimization\"}\n",
      "{\"author\": \"Beume, Nicola\", \"title\": \"SMS-EMOA: Multiobjective selection based on dominated hypervolume\"}\n",
      "{\"author\": \"Laumanns, M.\", \"title\": \"A unified model for multi-objective evolutionary algorithms with elitism\"}\n",
      "{\"author\": \"Auger, Anne\", \"title\": \"Theory of the hypervolume indicator: optimal \\u03bc-distributions and the choice of the reference point\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Ok, so I have this idea of \"evolving\" Gaussian distributions to fit a probabilistic model of some intrinsic dataset.\n",
    "\"\"\"\n",
    "\n",
    "# It tried, it should've replied with CMA-ES or MO-CMA-ES, but I guess the context length was too short.\n",
    "\n",
    "response, references = answer(prompt)\n",
    "print(response)\n",
    "print(references)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huawei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
